{"10:27": {"description": "        '''This searches 'buffer' for the first occurence of one of the search\n        strings.  'freshlen' must indicate the number of bytes at the end of\n        'buffer' which have not been searched before. It helps to avoid\n        searching the same, possibly big, buffer over and over again.\n\n        See class spawn for the 'searchwindowsize' argument.\n\n        If there is a match this returns the index of that string, and sets\n        'start', 'end' and 'match'. Otherwise, this returns -1. '''\n", "documentation": {}, "filename": "funcy/colls.py", "related_lines": {"4,90": []}, "commit_hash": "00863de7a8c5be2ac6503493e01f0b0281bab128", "repo_url": "https://github.com/Suor/funcy", "snippet_lines": ["    return _factory(coll)(ximap(f, iteritems(coll)))\n"], "start_line": 117}, "13:37": {"description": "        \"\"\"Delete a scheduled job.\"\"\"\n", "documentation": {}, "filename": "funcy/decorators.py", "related_lines": {"55,155": [], "8,42": []}, "commit_hash": "00863de7a8c5be2ac6503493e01f0b0281bab128", "repo_url": "https://github.com/Suor/funcy", "snippet_lines": ["        f = func  # remember the original func for error reporting\n", "        memo = set([id(f)]) # Memoise by id to tolerate non-hashable objects\n", "        while hasattr(func, '__wrapped__'):\n", "            func = func.__wrapped__\n", "            id_func = id(func)\n", "            if id_func in memo:\n", "                raise ValueError('wrapper loop when unwrapping {!r}'.format(f))\n", "            memo.add(id_func)\n", "        return func\n"], "start_line": 276}, "15:18": {"description": "        \"\"\"Look for a sequence of bytes at the start of a string. If the bytes\n        are found return True and advance the position to the byte after the\n        match. Otherwise return False and leave the position alone\"\"\"\n", "documentation": {"soup.find_all": "traverses the tree, starting at the given point, and finds all the Tag and NavigableString objects that match the criteria you give"}, "filename": "haul/finders/pipeline/html.py", "related_lines": {"4,43": []}, "commit_hash": "234024ab8452ea2f41b18561377295cf2879fb20", "repo_url": "https://github.com/vinta/Haul", "snippet_lines": ["    now_finder_image_urls = []\n", "\n", "    for img in soup.find_all('img'):\n", "        src = img.get('src', None)\n", "        if src:\n", "            src = str(src)\n", "            if (src not in finder_image_urls) and \\\n", "               (src not in now_finder_image_urls):\n", "                now_finder_image_urls.append(src)\n", "\n", "    output = {}\n", "    output['finder_image_urls'] = finder_image_urls + now_finder_image_urls\n", "\n", "    return output\n"], "start_line": 14}, "11:20": {"description": "        \"\"\"Loads the encoded object.  This function raises :class:`BadPayload`\n        if the payload is not valid.  The `serializer` parameter can be used to\n        override the serializer stored on the class.  The encoded payload is\n        always byte based.\n", "documentation": {}, "filename": "funcy/simple_funcs.py", "related_lines": {"37,66": []}, "commit_hash": "00863de7a8c5be2ac6503493e01f0b0281bab128", "repo_url": "https://github.com/Suor/funcy", "snippet_lines": ["    return lambda *a, **kw: func(*(args + a), **dict(kwargs, **kw))\n"], "start_line": 28}, "33:27": {"description": "        '''This searches 'buffer' for the first occurence of one of the search\n        strings.  'freshlen' must indicate the number of bytes at the end of\n        'buffer' which have not been searched before. It helps to avoid\n        searching the same, possibly big, buffer over and over again.\n\n        See class spawn for the 'searchwindowsize' argument.\n\n        If there is a match this returns the index of that string, and sets\n        'start', 'end' and 'match'. Otherwise, this returns -1. '''\n", "documentation": {}, "filename": "salt/pillar/s3.py", "related_lines": {"32,66": [], "4,30": []}, "commit_hash": "054539a830fe497f5b4fd58ffc5ccf04de7a3f04", "repo_url": "https://github.com/saltstack/salt", "snippet_lines": ["    cache_dir = os.path.join(__opts__['cachedir'], 'pillar_s3fs')\n", "\n", "    if not os.path.isdir(cache_dir):\n", "        log.debug('Initializing S3 Pillar Cache')\n", "        os.makedirs(cache_dir)\n", "\n", "    return cache_dir\n"], "start_line": 216}, "33:22": {"description": "            '''od.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n            If key is not found, d is returned if given, otherwise KeyError is raised.\n", "documentation": {}, "filename": "salt/pillar/s3.py", "related_lines": {"32,66": [], "4,30": []}, "commit_hash": "054539a830fe497f5b4fd58ffc5ccf04de7a3f04", "repo_url": "https://github.com/saltstack/salt", "snippet_lines": ["    cache_dir = os.path.join(__opts__['cachedir'], 'pillar_s3fs')\n", "\n", "    if not os.path.isdir(cache_dir):\n", "        log.debug('Initializing S3 Pillar Cache')\n", "        os.makedirs(cache_dir)\n", "\n", "    return cache_dir\n"], "start_line": 216}, "0:31": {"description": "    \"Converts a unix timestamp to a Python datetime object\"\n", "documentation": {}, "filename": "cartridge/shop/utils.py", "related_lines": {"4,56": []}, "commit_hash": "244bec069a12efc2ccf2efad6056a1b9323c39e3", "repo_url": "https://github.com/stephenmcd/cartridge", "snippet_lines": ["    for name in names:\n", "        try:\n", "            del request.session[name]\n", "        except KeyError:\n", "            pass\n"], "start_line": 31}, "24:19": {"description": "        \"\"\"Look for the next sequence of bytes matching a given sequence. If\n        a match is found advance the position to the last byte of the match\"\"\"\n", "documentation": {"import_from_str": "\n    Import and return an object defined as import string in the form of\n\n        path.to.module.object_name\n    "}, "filename": "model_mommy/utils.py", "related_lines": {"116,159": [], "4,56": []}, "commit_hash": "c7aa8d923f842d6961d8f56a6f34635c68352e87", "repo_url": "https://github.com/vandersonmota/model_mommy", "snippet_lines": ["    if isinstance(import_string_or_obj, string_types):\n", "        return import_from_str(import_string_or_obj)\n", "    return import_string_or_obj\n"], "start_line": 19}, "16:35": {"description": "        \"\"\"Run all jobs regardless if they are scheduled to run or not.\n\n\n\n        A delay of `delay` seconds is added between each job. This helps\n\n        distribute system load generated by the jobs more evenly\n\n        over time.\"\"\"\n", "documentation": {"treebuilders.getTreeBuilder": "Get a TreeBuilder class for various types of tree with built-in support\n\n    treeType - the name of the tree type required (case-insensitive). Supported\n               values are:\n\n               \"dom\" - A generic builder for DOM implementations, defaulting to\n                       a xml.dom.minidom based implementation.\n               \"etree\" - A generic builder for tree implementations exposing an\n                         ElementTree-like interface, defaulting to\n                         xml.etree.cElementTree if available and\n                         xml.etree.ElementTree if not.\n               \"lxml\" - A etree-based builder for lxml.etree, handling\n                        limitations of lxml's implementation.\n\n    implementation - (Currently applies to the \"etree\" and \"dom\" tree types). A\n                      module implementing the tree type e.g.\n                      xml.etree.ElementTree or xml.etree.cElementTree.", "parse": "Parse a HTML document into a well-formed tree", "HTMLParser": "HTML parser. Generates a tree structure from a stream of (possibly\n        malformed) HTML"}, "filename": "html5lib/html5parser.py", "related_lines": {"7,53": []}, "commit_hash": "a3022dcea691780d300547bbf68b4dd921995d1c", "repo_url": "https://github.com/html5lib/html5lib-python", "snippet_lines": ["    tb = treebuilders.getTreeBuilder(treebuilder)\n", "    p = HTMLParser(tb, namespaceHTMLElements=namespaceHTMLElements)\n", "    return p.parse(doc, **kwargs)\n"], "start_line": 33}, "23:41": {"description": "        \"\"\"Return ``True`` if this token is a whitespace token.\"\"\"\n", "documentation": {}, "filename": "model_mommy/mommy.py", "related_lines": {"131,199": [], "61,125": []}, "commit_hash": "c7aa8d923f842d6961d8f56a6f34635c68352e87", "repo_url": "https://github.com/vandersonmota/model_mommy", "snippet_lines": ["    rt = {}\n", "    if hasattr(generator, 'required'):\n", "        for item in generator.required:\n", "\n", "            if callable(item):  # mommy can deal with the nasty hacking too!\n", "                key, value = item(field)\n", "                rt[key] = value\n", "\n", "            elif isinstance(item, string_types):\n", "                rt[item] = getattr(field, item)\n", "\n", "            else:\n", "                raise ValueError(\"Required value '%s' is of wrong type. \\\n", "                                  Don't make mommy sad.\" % str(item))\n", "\n", "    return rt\n"], "start_line": 471}, "51:35": {"description": "        \"\"\"Run all jobs regardless if they are scheduled to run or not.\n\n\n\n        A delay of `delay` seconds is added between each job. This helps\n\n        distribute system load generated by the jobs more evenly\n\n        over time.\"\"\"\n", "documentation": {}, "filename": "tinydb/utils.py", "related_lines": {"8,47": []}, "commit_hash": "bc2b630dd1dd996a4aa0de07decb4c7ea0190b0e", "repo_url": "https://github.com/msiemens/tinydb", "snippet_lines": ["        if key in self.lru:\n", "            self.lru.remove(key)\n", "        self.lru.append(key)\n"], "start_line": 33}, "24:13": {"description": "        Get the object wrapped by ``func``.\n\n\n\n        Follows the chain of :attr:`__wrapped__` attributes returning the last\n\n        object in the chain.\n", "documentation": {"import_from_str": "\n    Import and return an object defined as import string in the form of\n\n        path.to.module.object_name\n    "}, "filename": "model_mommy/utils.py", "related_lines": {"116,159": [], "4,56": []}, "commit_hash": "c7aa8d923f842d6961d8f56a6f34635c68352e87", "repo_url": "https://github.com/vandersonmota/model_mommy", "snippet_lines": ["    if isinstance(import_string_or_obj, string_types):\n", "        return import_from_str(import_string_or_obj)\n", "    return import_string_or_obj\n"], "start_line": 19}, "19:46": {"description": "    Save the object to file via pickling.\n\n    Parameters\n    ----------\n    fname : str\n        Filename to pickle to\n", "documentation": {}, "filename": "html5lib/_inputstream.py", "related_lines": {"11,72": [], "74,152": []}, "commit_hash": "a3022dcea691780d300547bbf68b4dd921995d1c", "repo_url": "https://github.com/html5lib/html5lib-python", "snippet_lines": ["        newPosition = self[self.position:].find(bytes)\n", "        if newPosition > -1:\n", "            # XXX: This is ugly, but I can't see a nicer way to fix this.\n", "            if self._position == -1:\n", "                self._position = 0\n", "            self._position += (newPosition + len(bytes) - 1)\n", "            return True\n", "        else:\n", "            raise StopIteration\n"], "start_line": 677}, "39:51": {"description": "        Push a key to the tail of the LRU queue\n", "documentation": {"docs": "An iterable of document dictionaries."}, "filename": "solr/core.py", "related_lines": {"8,48": []}, "commit_hash": "ae72fb90b100090328aab1949ba978b1717795f5", "repo_url": "https://github.com/edsu/solrpy", "snippet_lines": ["        lst = [u'<add>']\n", "        for doc in docs:\n", "            self.__add(lst, doc)\n", "        lst.append(u'</add>')\n", "        return ''.join(lst)\n"], "start_line": 509}, "5:33": {"description": "    Get pillar cache directory. Initialize it if it does not exist.\n", "documentation": {}, "filename": "dejavu/fingerprint.py", "related_lines": {"6,40": [], "6,31": []}, "commit_hash": "4ab26bd0dbca3f349f1c44a308f204c6b1cb2d8a", "repo_url": "https://github.com/worldveil/dejavu", "snippet_lines": ["    frequency_idx = [x[1] for x in peaks_filtered]\n", "    time_idx = [x[0] for x in peaks_filtered]\n"], "start_line": 116}, "42:0": {"description": "    Removes values for the given session variables names\n    if they exist.\n", "documentation": {}, "filename": "sqlparse/sql.py", "related_lines": {"11,70": []}, "commit_hash": "b7e6ce7b7f687be884eea65df0e576c15b0331dc", "repo_url": "https://github.com/andialbrecht/sqlparse", "snippet_lines": ["        return self.parent == other\n"], "start_line": 119}, "3:18": {"description": "        \"\"\"Look for a sequence of bytes at the start of a string. If the bytes\n        are found return True and advance the position to the byte after the\n        match. Otherwise return False and leave the position alone\"\"\"\n", "documentation": {"hmac.new": "Create a new hashing object and return it.\n\n    key: The starting key for the hash.\n    msg: if available, will immediately be hashed into the object's starting\n    state.\n\n    You can now feed arbitrary strings into the object using its update()\n    method, and can ask for the hash value at any time by calling its digest()\n    method.\n    "}, "filename": "cartridge/shop/utils.py", "related_lines": {"4,39": []}, "commit_hash": "244bec069a12efc2ccf2efad6056a1b9323c39e3", "repo_url": "https://github.com/stephenmcd/cartridge", "snippet_lines": ["    key = bytes(settings.SECRET_KEY, encoding=\"utf8\")\n", "    value = bytes(value, encoding=\"utf8\")\n", "    return hmac.new(key, value, digest).hexdigest()\n"], "start_line": 94}, "19:50": {"description": "        Clear the query cache.\n\n        A simple helper that clears the internal query cache.\n", "documentation": {}, "filename": "html5lib/_inputstream.py", "related_lines": {"11,72": [], "74,152": []}, "commit_hash": "a3022dcea691780d300547bbf68b4dd921995d1c", "repo_url": "https://github.com/html5lib/html5lib-python", "snippet_lines": ["        newPosition = self[self.position:].find(bytes)\n", "        if newPosition > -1:\n", "            # XXX: This is ugly, but I can't see a nicer way to fix this.\n", "            if self._position == -1:\n", "                self._position = 0\n", "            self._position += (newPosition + len(bytes) - 1)\n", "            return True\n", "        else:\n", "            raise StopIteration\n"], "start_line": 677}, "33:5": {"description": "    # get indices for frequency and time\n", "documentation": {}, "filename": "salt/pillar/s3.py", "related_lines": {"32,66": [], "4,30": []}, "commit_hash": "054539a830fe497f5b4fd58ffc5ccf04de7a3f04", "repo_url": "https://github.com/saltstack/salt", "snippet_lines": ["    cache_dir = os.path.join(__opts__['cachedir'], 'pillar_s3fs')\n", "\n", "    if not os.path.isdir(cache_dir):\n", "        log.debug('Initializing S3 Pillar Cache')\n", "        os.makedirs(cache_dir)\n", "\n", "    return cache_dir\n"], "start_line": 216}, "34:42": {"description": "        \"\"\"Returns ``True`` if this token is a direct child of *other*.\"\"\"\n", "documentation": {"should_run": "True if the job should be run now", "job.should_run": "true if the job should be run now", "sorted": "Return a new sorted list from the items in iterable."}, "filename": "schedule/__init__.py", "related_lines": {"11,49": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        runnable_jobs = (job for job in self.jobs if job.should_run)\n", "        for job in sorted(runnable_jobs):\n", "            self._run_job(job)\n"], "start_line": 62}, "27:42": {"description": "        \"\"\"Returns ``True`` if this token is a direct child of *other*.\"\"\"\n", "documentation": {}, "filename": "pexpect/expect.py", "related_lines": {"432,471": [], "369,426": [], "473,499": []}, "commit_hash": "cb2f06f07d5a8dcd28f2d403a3a8f0845eae9861", "repo_url": "https://github.com/pexpect/pexpect", "snippet_lines": ["        first_match = None\n", "\n", "        # 'freshlen' helps a lot here. Further optimizations could\n", "        # possibly include:\n", "        #\n", "        # using something like the Boyer-Moore Fast String Searching\n", "        # Algorithm; pre-compiling the search through a list of\n", "        # strings into something that can scan the input once to\n", "        # search for all N strings; realize that if we search for\n", "        # ['bar', 'baz'] and the input is '...foo' we need not bother\n", "        # rescanning until we've read three more bytes.\n", "        #\n", "        # Sadly, I don't know enough about this interesting topic. /grahn\n", "\n", "        for index, s in self._strings:\n", "            if searchwindowsize is None:\n", "                # the match, if any, can only be in the fresh data,\n", "                # or at the very end of the old data\n", "                offset = -(freshlen + len(s))\n", "            else:\n", "                # better obey searchwindowsize\n", "                offset = -searchwindowsize\n", "            n = buffer.find(s, offset)\n", "            if n >= 0 and (first_match is None or n < first_match):\n", "                first_match = n\n", "                best_index, best_match = index, s\n", "        if first_match is None:\n", "            return -1\n", "        self.match = best_match\n", "        self.start = first_match\n", "        self.end = self.start + len(self.match)\n", "        return best_index\n"], "start_line": 174}, "0:3": {"description": "    Returns the hash of the given value, used for signing order key stored in\n    cookie for remembering address fields.\n", "documentation": {}, "filename": "cartridge/shop/utils.py", "related_lines": {"4,56": []}, "commit_hash": "244bec069a12efc2ccf2efad6056a1b9323c39e3", "repo_url": "https://github.com/stephenmcd/cartridge", "snippet_lines": ["    for name in names:\n", "        try:\n", "            del request.session[name]\n", "        except KeyError:\n", "            pass\n"], "start_line": 31}, "24:28": {"description": "        '''This calls write() for each element in the sequence.\n\n        The sequence can be any iterable object producing strings, typically a\n        list of strings. This does not add line separators. There is no return\n        value.\n", "documentation": {"import_from_str": "\n    Import and return an object defined as import string in the form of\n\n        path.to.module.object_name\n    "}, "filename": "model_mommy/utils.py", "related_lines": {"116,159": [], "4,56": []}, "commit_hash": "c7aa8d923f842d6961d8f56a6f34635c68352e87", "repo_url": "https://github.com/vandersonmota/model_mommy", "snippet_lines": ["    if isinstance(import_string_or_obj, string_types):\n", "        return import_from_str(import_string_or_obj)\n", "    return import_string_or_obj\n"], "start_line": 19}, "37:16": {"description": "    \"\"\"Parse a string or file-like object into a tree\"\"\"\n", "documentation": {"remove": "Remove the first occurrence of x from the array."}, "filename": "schedule/__init__.py", "related_lines": {"11,33": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        try:\n", "            self.jobs.remove(job)\n", "        except ValueError:\n", "            pass\n"], "start_line": 84}, "23:32": {"description": "        Return the value at key ``name``, raises a KeyError if the key\n        doesn't exist.\n", "documentation": {}, "filename": "model_mommy/mommy.py", "related_lines": {"131,199": [], "61,125": []}, "commit_hash": "c7aa8d923f842d6961d8f56a6f34635c68352e87", "repo_url": "https://github.com/vandersonmota/model_mommy", "snippet_lines": ["    rt = {}\n", "    if hasattr(generator, 'required'):\n", "        for item in generator.required:\n", "\n", "            if callable(item):  # mommy can deal with the nasty hacking too!\n", "                key, value = item(field)\n", "                rt[key] = value\n", "\n", "            elif isinstance(item, string_types):\n", "                rt[item] = getattr(field, item)\n", "\n", "            else:\n", "                raise ValueError(\"Required value '%s' is of wrong type. \\\n", "                                  Don't make mommy sad.\" % str(item))\n", "\n", "    return rt\n"], "start_line": 471}, "5:2": {"description": "    Stores the tax type and total in the session.\n", "documentation": {}, "filename": "dejavu/fingerprint.py", "related_lines": {"6,40": [], "6,31": []}, "commit_hash": "4ab26bd0dbca3f349f1c44a308f204c6b1cb2d8a", "repo_url": "https://github.com/worldveil/dejavu", "snippet_lines": ["    frequency_idx = [x[1] for x in peaks_filtered]\n", "    time_idx = [x[0] for x in peaks_filtered]\n"], "start_line": 116}, "37:1": {"description": "    Stores the shipping type and total in the session.\n", "documentation": {"remove": "Remove the first occurrence of x from the array."}, "filename": "schedule/__init__.py", "related_lines": {"11,33": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        try:\n", "            self.jobs.remove(job)\n", "        except ValueError:\n", "            pass\n"], "start_line": 84}, "5:23": {"description": "    Gets required values for a generator from the field.\n    If required value is a function, calls it with field as argument.\n    If required value is a string, simply fetch the value from the field\n    and return.\n", "documentation": {}, "filename": "dejavu/fingerprint.py", "related_lines": {"6,40": [], "6,31": []}, "commit_hash": "4ab26bd0dbca3f349f1c44a308f204c6b1cb2d8a", "repo_url": "https://github.com/worldveil/dejavu", "snippet_lines": ["    frequency_idx = [x[1] for x in peaks_filtered]\n", "    time_idx = [x[0] for x in peaks_filtered]\n"], "start_line": 116}, "2:19": {"description": "        \"\"\"Look for the next sequence of bytes matching a given sequence. If\n        a match is found advance the position to the last byte of the match\"\"\"\n", "documentation": {"_str": "\n    A backport of the Python 3 str object to Py2\n    "}, "filename": "cartridge/shop/utils.py", "related_lines": {"4,48": []}, "commit_hash": "244bec069a12efc2ccf2efad6056a1b9323c39e3", "repo_url": "https://github.com/stephenmcd/cartridge", "snippet_lines": ["    request.session[\"tax_type\"] = _str(tax_type)\n", "    request.session[\"tax_total\"] = _str(tax_total)\n"], "start_line": 85}, "30:25": {"description": "    Return a function that generates the toolbar tokens.\n", "documentation": {}, "filename": "pexpect/popen_spawn.py", "related_lines": {"11,32": []}, "commit_hash": "cb2f06f07d5a8dcd28f2d403a3a8f0845eae9861", "repo_url": "https://github.com/pexpect/pexpect", "snippet_lines": ["        self.proc.stdin.close()\n"], "start_line": 179}, "40:38": {"description": "        \"\"\"Schedule the job every day at a specific time.\n\n\n\n        Calling this is only valid for jobs scheduled to run every\n\n        N day(s).\n", "documentation": {}, "filename": "sqlparse/engine/statement_splitter.py", "related_lines": {"11,58": []}, "commit_hash": "b7e6ce7b7f687be884eea65df0e576c15b0331dc", "repo_url": "https://github.com/andialbrecht/sqlparse", "snippet_lines": ["        self._in_declare = False\n", "        self._in_dbldollar = False\n", "        self._is_create = False\n", "        self._begin_depth = 0\n", "\n", "        self.consume_ws = False\n", "        self.tokens = []\n", "        self.level = 0\n"], "start_line": 19}, "2:13": {"description": "        Get the object wrapped by ``func``.\n\n\n\n        Follows the chain of :attr:`__wrapped__` attributes returning the last\n\n        object in the chain.\n", "documentation": {"_str": "\n    A backport of the Python 3 str object to Py2\n    "}, "filename": "cartridge/shop/utils.py", "related_lines": {"4,48": []}, "commit_hash": "244bec069a12efc2ccf2efad6056a1b9323c39e3", "repo_url": "https://github.com/stephenmcd/cartridge", "snippet_lines": ["    request.session[\"tax_type\"] = _str(tax_type)\n", "    request.session[\"tax_total\"] = _str(tax_total)\n"], "start_line": 85}, "40:35": {"description": "        \"\"\"Run all jobs regardless if they are scheduled to run or not.\n\n\n\n        A delay of `delay` seconds is added between each job. This helps\n\n        distribute system load generated by the jobs more evenly\n\n        over time.\"\"\"\n", "documentation": {}, "filename": "sqlparse/engine/statement_splitter.py", "related_lines": {"11,58": []}, "commit_hash": "b7e6ce7b7f687be884eea65df0e576c15b0331dc", "repo_url": "https://github.com/andialbrecht/sqlparse", "snippet_lines": ["        self._in_declare = False\n", "        self._in_dbldollar = False\n", "        self._is_create = False\n", "        self._begin_depth = 0\n", "\n", "        self.consume_ws = False\n", "        self.tokens = []\n", "        self.level = 0\n"], "start_line": 19}, "28:14": {"description": "        Use requests to fetch remote content\n", "documentation": {"send": "send data to the subprocess' stdin. Returns the number of bytes written."}, "filename": "pexpect/popen_spawn.py", "related_lines": {"11,62": []}, "commit_hash": "cb2f06f07d5a8dcd28f2d403a3a8f0845eae9861", "repo_url": "https://github.com/pexpect/pexpect", "snippet_lines": ["        for s in sequence:\n", "            self.send(s)\n"], "start_line": 120}, "48:13": {"description": "        Get the object wrapped by ``func``.\n\n\n\n        Follows the chain of :attr:`__wrapped__` attributes returning the last\n\n        object in the chain.\n", "documentation": {}, "filename": "thefuck/main.py", "related_lines": {"7,37": []}, "commit_hash": "51415a5cb1ca6955fb99908e7d0e7bf012a66312", "repo_url": "https://github.com/nvbn/thefuck", "snippet_lines": ["    if entry_point:\n", "        warn('`thefuck-alias` is deprecated, use `thefuck --alias` instead.')\n", "        position = 1\n", "    else:\n", "        position = 2\n", "\n", "    alias = get_alias()\n", "    if len(sys.argv) > position:\n", "        alias = sys.argv[position]\n", "    print(shell.app_alias(alias))\n"], "start_line": 42}, "34:38": {"description": "        \"\"\"Schedule the job every day at a specific time.\n\n\n\n        Calling this is only valid for jobs scheduled to run every\n\n        N day(s).\n", "documentation": {"should_run": "True if the job should be run now", "job.should_run": "true if the job should be run now", "sorted": "Return a new sorted list from the items in iterable."}, "filename": "schedule/__init__.py", "related_lines": {"11,49": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        runnable_jobs = (job for job in self.jobs if job.should_run)\n", "        for job in sorted(runnable_jobs):\n", "            self._run_job(job)\n"], "start_line": 62}, "9:23": {"description": "    Gets required values for a generator from the field.\n    If required value is a function, calls it with field as argument.\n    If required value is a string, simply fetch the value from the field\n    and return.\n", "documentation": {"deque": "deque([iterable[, maxlen]]) --> deque object\n\nBuild an ordered collection with optimized access from its endpoints.", "izip": "izip(iter1 [,iter2 [...]]) --> izip object\n\nReturn a izip object whose .next() method returns a tuple where\nthe i-th element comes from the i-th iterable argument.  The .next()\nmethod continues until the shortest iterable in the argument sequence\nis exhausted and then it raises StopIteration.  Works like the zip()\nfunction but consumes less memory by returning an iterator instead of\na list."}, "filename": "funcy/seqs.py", "related_lines": {"4,50": [], "52,78": []}, "commit_hash": "00863de7a8c5be2ac6503493e01f0b0281bab128", "repo_url": "https://github.com/Suor/funcy", "snippet_lines": ["    counter = count()\n", "    deque(izip(seq, counter), maxlen=0)  # (consume at C speed)\n", "    return next(counter)\n"], "start_line": 91}, "47:24": {"description": "    Import and return an object defined as import string in the form of\n\n        path.to.module.object_name\n\n    or just return the object if it isn't a string.\n", "documentation": {"get_loaded_rules": "Yields all available rules.\n\n    :type rules_paths: [Path]\n    :rtype: Iterable[Rule]\n\n    "}, "filename": "thefuck/corrector.py", "related_lines": {"7,32": []}, "commit_hash": "51415a5cb1ca6955fb99908e7d0e7bf012a66312", "repo_url": "https://github.com/nvbn/thefuck", "snippet_lines": ["    bundled = Path(__file__).parent \\\n", "        .joinpath('rules') \\\n", "        .glob('*.py')\n", "    user = settings.user_dir.joinpath('rules').glob('*.py')\n", "    return sorted(get_loaded_rules(sorted(bundled) + sorted(user)),\n", "                  key=lambda rule: rule.priority)\n"], "start_line": 30}, "39:1": {"description": "    Stores the shipping type and total in the session.\n", "documentation": {"docs": "An iterable of document dictionaries."}, "filename": "solr/core.py", "related_lines": {"8,48": []}, "commit_hash": "ae72fb90b100090328aab1949ba978b1717795f5", "repo_url": "https://github.com/edsu/solrpy", "snippet_lines": ["        lst = [u'<add>']\n", "        for doc in docs:\n", "            self.__add(lst, doc)\n", "        lst.append(u'</add>')\n", "        return ''.join(lst)\n"], "start_line": 509}, "39:33": {"description": "    Get pillar cache directory. Initialize it if it does not exist.\n", "documentation": {"docs": "An iterable of document dictionaries."}, "filename": "solr/core.py", "related_lines": {"8,48": []}, "commit_hash": "ae72fb90b100090328aab1949ba978b1717795f5", "repo_url": "https://github.com/edsu/solrpy", "snippet_lines": ["        lst = [u'<add>']\n", "        for doc in docs:\n", "            self.__add(lst, doc)\n", "        lst.append(u'</add>')\n", "        return ''.join(lst)\n"], "start_line": 509}, "47:1": {"description": "    Stores the shipping type and total in the session.\n", "documentation": {"get_loaded_rules": "Yields all available rules.\n\n    :type rules_paths: [Path]\n    :rtype: Iterable[Rule]\n\n    "}, "filename": "thefuck/corrector.py", "related_lines": {"7,32": []}, "commit_hash": "51415a5cb1ca6955fb99908e7d0e7bf012a66312", "repo_url": "https://github.com/nvbn/thefuck", "snippet_lines": ["    bundled = Path(__file__).parent \\\n", "        .joinpath('rules') \\\n", "        .glob('*.py')\n", "    user = settings.user_dir.joinpath('rules').glob('*.py')\n", "    return sorted(get_loaded_rules(sorted(bundled) + sorted(user)),\n", "                  key=lambda rule: rule.priority)\n"], "start_line": 30}, "13:10": {"description": "    Walk coll transforming it's elements with f.\n\n    Same as map, but preserves coll type.\n", "documentation": {}, "filename": "funcy/decorators.py", "related_lines": {"55,155": [], "8,42": []}, "commit_hash": "00863de7a8c5be2ac6503493e01f0b0281bab128", "repo_url": "https://github.com/Suor/funcy", "snippet_lines": ["        f = func  # remember the original func for error reporting\n", "        memo = set([id(f)]) # Memoise by id to tolerate non-hashable objects\n", "        while hasattr(func, '__wrapped__'):\n", "            func = func.__wrapped__\n", "            id_func = id(func)\n", "            if id_func in memo:\n", "                raise ValueError('wrapper loop when unwrapping {!r}'.format(f))\n", "            memo.add(id_func)\n", "        return func\n"], "start_line": 276}, "21:24": {"description": "    Import and return an object defined as import string in the form of\n\n        path.to.module.object_name\n\n    or just return the object if it isn't a string.\n", "documentation": {}, "filename": "micawber/compat.py", "related_lines": {"39,76": []}, "commit_hash": "749f32825eedcdea69be9eb874d9ded0bd227dc3", "repo_url": "https://github.com/coleifer/micawber", "snippet_lines": ["            if not self:\n", "                raise KeyError('dictionary is empty')\n", "            root = self.__root\n", "            if last:\n", "                link = root[0]\n", "                link_prev = link[0]\n", "                link_prev[1] = root\n", "                root[0] = link_prev\n", "            else:\n", "                link = root[1]\n", "                link_next = link[1]\n", "                root[1] = link_next\n", "                link_next[0] = root\n", "            key = link[2]\n", "            del self.__map[key]\n", "            value = dict.pop(self, key)\n", "            return key, value\n"], "start_line": 108}, "46:33": {"description": "    Get pillar cache directory. Initialize it if it does not exist.\n", "documentation": {"cPickle.dump": "dump(obj, file, protocol=0) -- Write an object in pickle format to the given file.\n\nSee the Pickler docstring for the meaning of optional argument proto.", "get_file_obj": "\n    Light wrapper to handle strings and let files (anything else) pass through.\n\n    It also handle '.gz' files.\n\n    Parameters\n    ==========\n    fname: string or file-like object\n        File to open / forward\n    mode: string\n        Argument passed to the 'open' or 'gzip.open' function\n    encoding: string\n        For Python 3 only, specify the encoding of the file\n\n    Returns\n    =======\n    A file-like object that is always a context-manager. If the `fname` was already a file-like object,\n    the returned context manager *will not close the file*.\n    "}, "filename": "statsmodels/iolib/smpickle.py", "related_lines": {"4,40": []}, "commit_hash": "036ad9f769068a50d48048c889762a1e32e5eb1a", "repo_url": "https://github.com/statsmodels/statsmodels", "snippet_lines": ["    with get_file_obj(fname, 'wb') as fout:\n", "        cPickle.dump(obj, fout, protocol=-1)\n"], "start_line": 14}, "47:9": {"description": "    Consume an iterable not reading it into memory; return the number of items.\n", "documentation": {"get_loaded_rules": "Yields all available rules.\n\n    :type rules_paths: [Path]\n    :rtype: Iterable[Rule]\n\n    "}, "filename": "thefuck/corrector.py", "related_lines": {"7,32": []}, "commit_hash": "51415a5cb1ca6955fb99908e7d0e7bf012a66312", "repo_url": "https://github.com/nvbn/thefuck", "snippet_lines": ["    bundled = Path(__file__).parent \\\n", "        .joinpath('rules') \\\n", "        .glob('*.py')\n", "    user = settings.user_dir.joinpath('rules').glob('*.py')\n", "    return sorted(get_loaded_rules(sorted(bundled) + sorted(user)),\n", "                  key=lambda rule: rule.priority)\n"], "start_line": 30}, "42:16": {"description": "    \"\"\"Parse a string or file-like object into a tree\"\"\"\n", "documentation": {}, "filename": "sqlparse/sql.py", "related_lines": {"11,70": []}, "commit_hash": "b7e6ce7b7f687be884eea65df0e576c15b0331dc", "repo_url": "https://github.com/andialbrecht/sqlparse", "snippet_lines": ["        return self.parent == other\n"], "start_line": 119}, "37:27": {"description": "        '''This searches 'buffer' for the first occurence of one of the search\n        strings.  'freshlen' must indicate the number of bytes at the end of\n        'buffer' which have not been searched before. It helps to avoid\n        searching the same, possibly big, buffer over and over again.\n\n        See class spawn for the 'searchwindowsize' argument.\n\n        If there is a match this returns the index of that string, and sets\n        'start', 'end' and 'match'. Otherwise, this returns -1. '''\n", "documentation": {"remove": "Remove the first occurrence of x from the array."}, "filename": "schedule/__init__.py", "related_lines": {"11,33": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        try:\n", "            self.jobs.remove(job)\n", "        except ValueError:\n", "            pass\n"], "start_line": 84}, "11:40": {"description": "        \"\"\"Set the filter attributes to its default values\"\"\"\n", "documentation": {}, "filename": "funcy/simple_funcs.py", "related_lines": {"37,66": []}, "commit_hash": "00863de7a8c5be2ac6503493e01f0b0281bab128", "repo_url": "https://github.com/Suor/funcy", "snippet_lines": ["    return lambda *a, **kw: func(*(args + a), **dict(kwargs, **kw))\n"], "start_line": 28}, "43:6": {"description": "        '''Read up to `amt` bytes off the pipeline.\n\n        May raise PipelineCloseRequest if the pipeline is\n        empty and the connected stream should be closed.\n", "documentation": {}, "filename": "sqlparse/sql.py", "related_lines": {"11,65": []}, "commit_hash": "b7e6ce7b7f687be884eea65df0e576c15b0331dc", "repo_url": "https://github.com/andialbrecht/sqlparse", "snippet_lines": ["        parent = self.parent\n", "        while parent:\n", "            if parent == other:\n", "                return True\n", "            parent = parent.parent\n", "        return False\n"], "start_line": 123}, "0:11": {"description": "    A functools.partial alternative, which returns a real function.\n\n    Can be used to construct methods.\n", "documentation": {}, "filename": "cartridge/shop/utils.py", "related_lines": {"4,56": []}, "commit_hash": "244bec069a12efc2ccf2efad6056a1b9323c39e3", "repo_url": "https://github.com/stephenmcd/cartridge", "snippet_lines": ["    for name in names:\n", "        try:\n", "            del request.session[name]\n", "        except KeyError:\n", "            pass\n"], "start_line": 31}, "37:29": {"description": "        '''Wait for the subprocess to finish.\n        \n        Returns the exit code.\n", "documentation": {"remove": "Remove the first occurrence of x from the array."}, "filename": "schedule/__init__.py", "related_lines": {"11,33": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        try:\n", "            self.jobs.remove(job)\n", "        except ValueError:\n", "            pass\n"], "start_line": 84}, "40:31": {"description": "    \"Converts a unix timestamp to a Python datetime object\"\n", "documentation": {}, "filename": "sqlparse/engine/statement_splitter.py", "related_lines": {"11,58": []}, "commit_hash": "b7e6ce7b7f687be884eea65df0e576c15b0331dc", "repo_url": "https://github.com/andialbrecht/sqlparse", "snippet_lines": ["        self._in_declare = False\n", "        self._in_dbldollar = False\n", "        self._is_create = False\n", "        self._begin_depth = 0\n", "\n", "        self.consume_ws = False\n", "        self.tokens = []\n", "        self.level = 0\n"], "start_line": 19}, "21:11": {"description": "    A functools.partial alternative, which returns a real function.\n\n    Can be used to construct methods.\n", "documentation": {}, "filename": "micawber/compat.py", "related_lines": {"39,76": []}, "commit_hash": "749f32825eedcdea69be9eb874d9ded0bd227dc3", "repo_url": "https://github.com/coleifer/micawber", "snippet_lines": ["            if not self:\n", "                raise KeyError('dictionary is empty')\n", "            root = self.__root\n", "            if last:\n", "                link = root[0]\n", "                link_prev = link[0]\n", "                link_prev[1] = root\n", "                root[0] = link_prev\n", "            else:\n", "                link = root[1]\n", "                link_next = link[1]\n", "                root[1] = link_next\n", "                link_next[0] = root\n", "            key = link[2]\n", "            del self.__map[key]\n", "            value = dict.pop(self, key)\n", "            return key, value\n"], "start_line": 108}, "21:15": {"description": "    Find image URL in <img>'s src attribute\n", "documentation": {}, "filename": "micawber/compat.py", "related_lines": {"39,76": []}, "commit_hash": "749f32825eedcdea69be9eb874d9ded0bd227dc3", "repo_url": "https://github.com/coleifer/micawber", "snippet_lines": ["            if not self:\n", "                raise KeyError('dictionary is empty')\n", "            root = self.__root\n", "            if last:\n", "                link = root[0]\n", "                link_prev = link[0]\n", "                link_prev[1] = root\n", "                root[0] = link_prev\n", "            else:\n", "                link = root[1]\n", "                link_next = link[1]\n", "                root[1] = link_next\n", "                link_next[0] = root\n", "            key = link[2]\n", "            del self.__map[key]\n", "            value = dict.pop(self, key)\n", "            return key, value\n"], "start_line": 108}, "28:8": {"description": "    Stop and disable a service (convenience function).\n\n    .. note:: This function is idempotent.\n", "documentation": {"send": "send data to the subprocess' stdin. Returns the number of bytes written."}, "filename": "pexpect/popen_spawn.py", "related_lines": {"11,62": []}, "commit_hash": "cb2f06f07d5a8dcd28f2d403a3a8f0845eae9861", "repo_url": "https://github.com/pexpect/pexpect", "snippet_lines": ["        for s in sequence:\n", "            self.send(s)\n"], "start_line": 120}, "15:1": {"description": "    Stores the shipping type and total in the session.\n", "documentation": {"soup.find_all": "traverses the tree, starting at the given point, and finds all the Tag and NavigableString objects that match the criteria you give"}, "filename": "haul/finders/pipeline/html.py", "related_lines": {"4,43": []}, "commit_hash": "234024ab8452ea2f41b18561377295cf2879fb20", "repo_url": "https://github.com/vinta/Haul", "snippet_lines": ["    now_finder_image_urls = []\n", "\n", "    for img in soup.find_all('img'):\n", "        src = img.get('src', None)\n", "        if src:\n", "            src = str(src)\n", "            if (src not in finder_image_urls) and \\\n", "               (src not in now_finder_image_urls):\n", "                now_finder_image_urls.append(src)\n", "\n", "    output = {}\n", "    output['finder_image_urls'] = finder_image_urls + now_finder_image_urls\n", "\n", "    return output\n"], "start_line": 14}, "19:8": {"description": "    Stop and disable a service (convenience function).\n\n    .. note:: This function is idempotent.\n", "documentation": {}, "filename": "html5lib/_inputstream.py", "related_lines": {"11,72": [], "74,152": []}, "commit_hash": "a3022dcea691780d300547bbf68b4dd921995d1c", "repo_url": "https://github.com/html5lib/html5lib-python", "snippet_lines": ["        newPosition = self[self.position:].find(bytes)\n", "        if newPosition > -1:\n", "            # XXX: This is ugly, but I can't see a nicer way to fix this.\n", "            if self._position == -1:\n", "                self._position = 0\n", "            self._position += (newPosition + len(bytes) - 1)\n", "            return True\n", "        else:\n", "            raise StopIteration\n"], "start_line": 677}, "30:4": {"description": "    # return hashes\n", "documentation": {}, "filename": "pexpect/popen_spawn.py", "related_lines": {"11,32": []}, "commit_hash": "cb2f06f07d5a8dcd28f2d403a3a8f0845eae9861", "repo_url": "https://github.com/pexpect/pexpect", "snippet_lines": ["        self.proc.stdin.close()\n"], "start_line": 179}, "47:50": {"description": "        Clear the query cache.\n\n        A simple helper that clears the internal query cache.\n", "documentation": {"get_loaded_rules": "Yields all available rules.\n\n    :type rules_paths: [Path]\n    :rtype: Iterable[Rule]\n\n    "}, "filename": "thefuck/corrector.py", "related_lines": {"7,32": []}, "commit_hash": "51415a5cb1ca6955fb99908e7d0e7bf012a66312", "repo_url": "https://github.com/nvbn/thefuck", "snippet_lines": ["    bundled = Path(__file__).parent \\\n", "        .joinpath('rules') \\\n", "        .glob('*.py')\n", "    user = settings.user_dir.joinpath('rules').glob('*.py')\n", "    return sorted(get_loaded_rules(sorted(bundled) + sorted(user)),\n", "                  key=lambda rule: rule.priority)\n"], "start_line": 30}, "8:49": {"description": "        Get access to a specific table.\n\n        Creates a new table, if it hasn't been created before, otherwise it\n        returns the cached :class:`~tinydb.Table` object.\n\n        :param name: The name of the table.\n        :type name: str\n        :param cache_size: How many query results to cache.\n", "documentation": {"disable": "\n    Disable a service.\n\n    ::\n\n        fabtools.systemd.disable('httpd')\n\n    .. note:: This function is idempotent.\n    ", "stop": "\n    Stop a service.\n\n    ::\n\n        if fabtools.systemd.is_running('foo'):\n            fabtools.systemd.stop('foo')\n\n    .. note:: This function is idempotent.\n    "}, "filename": "fabtools/systemd.py", "related_lines": {"4,30": []}, "commit_hash": "f7e851efaab057becce4ce0b8bd5fba146bf34c3", "repo_url": "https://github.com/ronnix/fabtools", "snippet_lines": ["    stop(service)\n", "    disable(service)\n"], "start_line": 132}, "34:24": {"description": "    Import and return an object defined as import string in the form of\n\n        path.to.module.object_name\n\n    or just return the object if it isn't a string.\n", "documentation": {"should_run": "True if the job should be run now", "job.should_run": "true if the job should be run now", "sorted": "Return a new sorted list from the items in iterable."}, "filename": "schedule/__init__.py", "related_lines": {"11,49": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        runnable_jobs = (job for job in self.jobs if job.should_run)\n", "        for job in sorted(runnable_jobs):\n", "            self._run_job(job)\n"], "start_line": 62}, "7:15": {"description": "    Find image URL in <img>'s src attribute\n", "documentation": {}, "filename": "fabtools/systemd.py", "related_lines": {"4,20": []}, "commit_hash": "f7e851efaab057becce4ce0b8bd5fba146bf34c3", "repo_url": "https://github.com/ronnix/fabtools", "snippet_lines": ["    action('enable', service)\n"], "start_line": 30}, "26:10": {"description": "    Walk coll transforming it's elements with f.\n\n    Same as map, but preserves coll type.\n", "documentation": {"ConfigObj": "An object to read, create, and write config files.", "merge": "This method is a recursive update method. It allows you to merge two config files together.", "read_config_file": "Read a config file."}, "filename": "mycli/config.py", "related_lines": {"7,44": []}, "commit_hash": "a69bacce987dc8e4c5a5de71b882dbbf8949e916", "repo_url": "https://github.com/dbcli/mycli", "snippet_lines": ["    config = ConfigObj()\n", "\n", "    for _file in files:\n", "        _config = read_config_file(_file)\n", "        if bool(_config) is True:\n", "            config.merge(_config)\n", "            config.filename = _config.filename\n", "\n", "    return config\n"], "start_line": 59}, "7:17": {"description": "        \"\"\"Attempts to detect at BOM at the start of the stream. If\n        an encoding can be determined from the BOM return the name of the\n        encoding otherwise return None\"\"\"\n", "documentation": {}, "filename": "fabtools/systemd.py", "related_lines": {"4,20": []}, "commit_hash": "f7e851efaab057becce4ce0b8bd5fba146bf34c3", "repo_url": "https://github.com/ronnix/fabtools", "snippet_lines": ["    action('enable', service)\n"], "start_line": 30}, "12:51": {"description": "        Push a key to the tail of the LRU queue\n", "documentation": {}, "filename": "funcy/decorators.py", "related_lines": {"15,50": []}, "commit_hash": "00863de7a8c5be2ac6503493e01f0b0281bab128", "repo_url": "https://github.com/Suor/funcy", "snippet_lines": ["            return self\n"], "start_line": 106}, "51:11": {"description": "    A functools.partial alternative, which returns a real function.\n\n    Can be used to construct methods.\n", "documentation": {}, "filename": "tinydb/utils.py", "related_lines": {"8,47": []}, "commit_hash": "bc2b630dd1dd996a4aa0de07decb4c7ea0190b0e", "repo_url": "https://github.com/msiemens/tinydb", "snippet_lines": ["        if key in self.lru:\n", "            self.lru.remove(key)\n", "        self.lru.append(key)\n"], "start_line": 33}, "23:14": {"description": "        Use requests to fetch remote content\n", "documentation": {}, "filename": "model_mommy/mommy.py", "related_lines": {"131,199": [], "61,125": []}, "commit_hash": "c7aa8d923f842d6961d8f56a6f34635c68352e87", "repo_url": "https://github.com/vandersonmota/model_mommy", "snippet_lines": ["    rt = {}\n", "    if hasattr(generator, 'required'):\n", "        for item in generator.required:\n", "\n", "            if callable(item):  # mommy can deal with the nasty hacking too!\n", "                key, value = item(field)\n", "                rt[key] = value\n", "\n", "            elif isinstance(item, string_types):\n", "                rt[item] = getattr(field, item)\n", "\n", "            else:\n", "                raise ValueError(\"Required value '%s' is of wrong type. \\\n", "                                  Don't make mommy sad.\" % str(item))\n", "\n", "    return rt\n"], "start_line": 471}, "37:31": {"description": "    \"Converts a unix timestamp to a Python datetime object\"\n", "documentation": {"remove": "Remove the first occurrence of x from the array."}, "filename": "schedule/__init__.py", "related_lines": {"11,33": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        try:\n", "            self.jobs.remove(job)\n", "        except ValueError:\n", "            pass\n"], "start_line": 84}, "28:47": {"description": "    \"\"\"Returns all enabled rules.\n\n    :rtype: [Rule]\n", "documentation": {"send": "send data to the subprocess' stdin. Returns the number of bytes written."}, "filename": "pexpect/popen_spawn.py", "related_lines": {"11,62": []}, "commit_hash": "cb2f06f07d5a8dcd28f2d403a3a8f0845eae9861", "repo_url": "https://github.com/pexpect/pexpect", "snippet_lines": ["        for s in sequence:\n", "            self.send(s)\n"], "start_line": 120}, "17:24": {"description": "    Import and return an object defined as import string in the form of\n\n        path.to.module.object_name\n\n    or just return the object if it isn't a string.\n", "documentation": {"lookupEncoding": "Return the python codec name corresponding to an encoding or None if the\n    string doesn't correspond to a valid encoding."}, "filename": "html5lib/_inputstream.py", "related_lines": {"159,180": [], "11,63": [], "65,158": []}, "commit_hash": "a3022dcea691780d300547bbf68b4dd921995d1c", "repo_url": "https://github.com/html5lib/html5lib-python", "snippet_lines": ["        bomDict = {\n", "            codecs.BOM_UTF8: 'utf-8',\n", "            codecs.BOM_UTF16_LE: 'utf-16le', codecs.BOM_UTF16_BE: 'utf-16be',\n", "            codecs.BOM_UTF32_LE: 'utf-32le', codecs.BOM_UTF32_BE: 'utf-32be'\n", "        }\n", "\n", "        # Go to beginning of file and read in 4 bytes\n", "        string = self.rawStream.read(4)\n", "        assert isinstance(string, bytes)\n", "\n", "        # Try detecting the BOM using bytes from the string\n", "        encoding = bomDict.get(string[:3])         # UTF-8\n", "        seek = 3\n", "        if not encoding:\n", "            # Need to detect UTF-32 before UTF-16\n", "            encoding = bomDict.get(string)         # UTF-32\n", "            seek = 4\n", "            if not encoding:\n", "                encoding = bomDict.get(string[:2])  # UTF-16\n", "                seek = 2\n", "\n", "        # Set the read position past the BOM if one was found, otherwise\n", "        # set it to the start of the stream\n", "        if encoding:\n", "            self.rawStream.seek(seek)\n", "            return lookupEncoding(encoding)\n", "        else:\n", "            self.rawStream.seek(0)\n", "            return None\n"], "start_line": 539}, "34:32": {"description": "        Return the value at key ``name``, raises a KeyError if the key\n        doesn't exist.\n", "documentation": {"should_run": "True if the job should be run now", "job.should_run": "true if the job should be run now", "sorted": "Return a new sorted list from the items in iterable."}, "filename": "schedule/__init__.py", "related_lines": {"11,49": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        runnable_jobs = (job for job in self.jobs if job.should_run)\n", "        for job in sorted(runnable_jobs):\n", "            self._run_job(job)\n"], "start_line": 62}, "34:18": {"description": "        \"\"\"Look for a sequence of bytes at the start of a string. If the bytes\n        are found return True and advance the position to the byte after the\n        match. Otherwise return False and leave the position alone\"\"\"\n", "documentation": {"should_run": "True if the job should be run now", "job.should_run": "true if the job should be run now", "sorted": "Return a new sorted list from the items in iterable."}, "filename": "schedule/__init__.py", "related_lines": {"11,49": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        runnable_jobs = (job for job in self.jobs if job.should_run)\n", "        for job in sorted(runnable_jobs):\n", "            self._run_job(job)\n"], "start_line": 62}, "13:9": {"description": "    Consume an iterable not reading it into memory; return the number of items.\n", "documentation": {}, "filename": "funcy/decorators.py", "related_lines": {"55,155": [], "8,42": []}, "commit_hash": "00863de7a8c5be2ac6503493e01f0b0281bab128", "repo_url": "https://github.com/Suor/funcy", "snippet_lines": ["        f = func  # remember the original func for error reporting\n", "        memo = set([id(f)]) # Memoise by id to tolerate non-hashable objects\n", "        while hasattr(func, '__wrapped__'):\n", "            func = func.__wrapped__\n", "            id_func = id(func)\n", "            if id_func in memo:\n", "                raise ValueError('wrapper loop when unwrapping {!r}'.format(f))\n", "            memo.add(id_func)\n", "        return func\n"], "start_line": 276}, "43:39": {"description": "        Add several documents to the Solr server.\n\n        `docs`\n            An iterable of document dictionaries.\n\n        Supports commit-control arguments.\n", "documentation": {}, "filename": "sqlparse/sql.py", "related_lines": {"11,65": []}, "commit_hash": "b7e6ce7b7f687be884eea65df0e576c15b0331dc", "repo_url": "https://github.com/andialbrecht/sqlparse", "snippet_lines": ["        parent = self.parent\n", "        while parent:\n", "            if parent == other:\n", "                return True\n", "            parent = parent.parent\n", "        return False\n"], "start_line": 123}, "46:2": {"description": "    Stores the tax type and total in the session.\n", "documentation": {"cPickle.dump": "dump(obj, file, protocol=0) -- Write an object in pickle format to the given file.\n\nSee the Pickler docstring for the meaning of optional argument proto.", "get_file_obj": "\n    Light wrapper to handle strings and let files (anything else) pass through.\n\n    It also handle '.gz' files.\n\n    Parameters\n    ==========\n    fname: string or file-like object\n        File to open / forward\n    mode: string\n        Argument passed to the 'open' or 'gzip.open' function\n    encoding: string\n        For Python 3 only, specify the encoding of the file\n\n    Returns\n    =======\n    A file-like object that is always a context-manager. If the `fname` was already a file-like object,\n    the returned context manager *will not close the file*.\n    "}, "filename": "statsmodels/iolib/smpickle.py", "related_lines": {"4,40": []}, "commit_hash": "036ad9f769068a50d48048c889762a1e32e5eb1a", "repo_url": "https://github.com/statsmodels/statsmodels", "snippet_lines": ["    with get_file_obj(fname, 'wb') as fout:\n", "        cPickle.dump(obj, fout, protocol=-1)\n"], "start_line": 14}, "13:7": {"description": "    Enable a service.\n\n    ::\n\n        fabtools.enable('httpd')\n\n    .. note:: This function is idempotent.\n", "documentation": {}, "filename": "funcy/decorators.py", "related_lines": {"55,155": [], "8,42": []}, "commit_hash": "00863de7a8c5be2ac6503493e01f0b0281bab128", "repo_url": "https://github.com/Suor/funcy", "snippet_lines": ["        f = func  # remember the original func for error reporting\n", "        memo = set([id(f)]) # Memoise by id to tolerate non-hashable objects\n", "        while hasattr(func, '__wrapped__'):\n", "            func = func.__wrapped__\n", "            id_func = id(func)\n", "            if id_func in memo:\n", "                raise ValueError('wrapper loop when unwrapping {!r}'.format(f))\n", "            memo.add(id_func)\n", "        return func\n"], "start_line": 276}, "29:39": {"description": "        Add several documents to the Solr server.\n\n        `docs`\n            An iterable of document dictionaries.\n\n        Supports commit-control arguments.\n", "documentation": {}, "filename": "pexpect/popen_spawn.py", "related_lines": {"63,84": [], "11,44": []}, "commit_hash": "cb2f06f07d5a8dcd28f2d403a3a8f0845eae9861", "repo_url": "https://github.com/pexpect/pexpect", "snippet_lines": ["        status = self.proc.wait()\n", "        if status >= 0:\n", "            self.exitstatus = status\n", "            self.signalstatus = None\n", "        else:\n", "            self.exitstatus = None\n", "            self.signalstatus = -status\n", "        self.terminated = True\n", "        return status\n"], "start_line": 152}, "29:50": {"description": "        Clear the query cache.\n\n        A simple helper that clears the internal query cache.\n", "documentation": {}, "filename": "pexpect/popen_spawn.py", "related_lines": {"63,84": [], "11,44": []}, "commit_hash": "cb2f06f07d5a8dcd28f2d403a3a8f0845eae9861", "repo_url": "https://github.com/pexpect/pexpect", "snippet_lines": ["        status = self.proc.wait()\n", "        if status >= 0:\n", "            self.exitstatus = status\n", "            self.signalstatus = None\n", "        else:\n", "            self.exitstatus = None\n", "            self.signalstatus = -status\n", "        self.terminated = True\n", "        return status\n"], "start_line": 152}, "47:4": {"description": "    # return hashes\n", "documentation": {"get_loaded_rules": "Yields all available rules.\n\n    :type rules_paths: [Path]\n    :rtype: Iterable[Rule]\n\n    "}, "filename": "thefuck/corrector.py", "related_lines": {"7,32": []}, "commit_hash": "51415a5cb1ca6955fb99908e7d0e7bf012a66312", "repo_url": "https://github.com/nvbn/thefuck", "snippet_lines": ["    bundled = Path(__file__).parent \\\n", "        .joinpath('rules') \\\n", "        .glob('*.py')\n", "    user = settings.user_dir.joinpath('rules').glob('*.py')\n", "    return sorted(get_loaded_rules(sorted(bundled) + sorted(user)),\n", "                  key=lambda rule: rule.priority)\n"], "start_line": 30}, "49:22": {"description": "            '''od.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n            If key is not found, d is returned if given, otherwise KeyError is raised.\n", "documentation": {"_read": "Reading access to the DB."}, "filename": "tinydb/database.py", "related_lines": {"104,173": [], "49,102": []}, "commit_hash": "bc2b630dd1dd996a4aa0de07decb4c7ea0190b0e", "repo_url": "https://github.com/msiemens/tinydb", "snippet_lines": ["        if name in self._table_cache:\n", "            return self._table_cache[name]\n", "\n", "        table = self.table_class(StorageProxy(self._storage, name), **options)\n", "\n", "        self._table_cache[name] = table\n", "        \n", "        # table._read will create an empty table in the storage, if necessary\n", "        table._read()\n", "\n", "        return table\n"], "start_line": 107}, "27:18": {"description": "        \"\"\"Look for a sequence of bytes at the start of a string. If the bytes\n        are found return True and advance the position to the byte after the\n        match. Otherwise return False and leave the position alone\"\"\"\n", "documentation": {}, "filename": "pexpect/expect.py", "related_lines": {"432,471": [], "369,426": [], "473,499": []}, "commit_hash": "cb2f06f07d5a8dcd28f2d403a3a8f0845eae9861", "repo_url": "https://github.com/pexpect/pexpect", "snippet_lines": ["        first_match = None\n", "\n", "        # 'freshlen' helps a lot here. Further optimizations could\n", "        # possibly include:\n", "        #\n", "        # using something like the Boyer-Moore Fast String Searching\n", "        # Algorithm; pre-compiling the search through a list of\n", "        # strings into something that can scan the input once to\n", "        # search for all N strings; realize that if we search for\n", "        # ['bar', 'baz'] and the input is '...foo' we need not bother\n", "        # rescanning until we've read three more bytes.\n", "        #\n", "        # Sadly, I don't know enough about this interesting topic. /grahn\n", "\n", "        for index, s in self._strings:\n", "            if searchwindowsize is None:\n", "                # the match, if any, can only be in the fresh data,\n", "                # or at the very end of the old data\n", "                offset = -(freshlen + len(s))\n", "            else:\n", "                # better obey searchwindowsize\n", "                offset = -searchwindowsize\n", "            n = buffer.find(s, offset)\n", "            if n >= 0 and (first_match is None or n < first_match):\n", "                first_match = n\n", "                best_index, best_match = index, s\n", "        if first_match is None:\n", "            return -1\n", "        self.match = best_match\n", "        self.start = first_match\n", "        self.end = self.start + len(self.match)\n", "        return best_index\n"], "start_line": 174}, "51:27": {"description": "        '''This searches 'buffer' for the first occurence of one of the search\n        strings.  'freshlen' must indicate the number of bytes at the end of\n        'buffer' which have not been searched before. It helps to avoid\n        searching the same, possibly big, buffer over and over again.\n\n        See class spawn for the 'searchwindowsize' argument.\n\n        If there is a match this returns the index of that string, and sets\n        'start', 'end' and 'match'. Otherwise, this returns -1. '''\n", "documentation": {}, "filename": "tinydb/utils.py", "related_lines": {"8,47": []}, "commit_hash": "bc2b630dd1dd996a4aa0de07decb4c7ea0190b0e", "repo_url": "https://github.com/msiemens/tinydb", "snippet_lines": ["        if key in self.lru:\n", "            self.lru.remove(key)\n", "        self.lru.append(key)\n"], "start_line": 33}, "42:39": {"description": "        Add several documents to the Solr server.\n\n        `docs`\n            An iterable of document dictionaries.\n\n        Supports commit-control arguments.\n", "documentation": {}, "filename": "sqlparse/sql.py", "related_lines": {"11,70": []}, "commit_hash": "b7e6ce7b7f687be884eea65df0e576c15b0331dc", "repo_url": "https://github.com/andialbrecht/sqlparse", "snippet_lines": ["        return self.parent == other\n"], "start_line": 119}, "27:10": {"description": "    Walk coll transforming it's elements with f.\n\n    Same as map, but preserves coll type.\n", "documentation": {}, "filename": "pexpect/expect.py", "related_lines": {"432,471": [], "369,426": [], "473,499": []}, "commit_hash": "cb2f06f07d5a8dcd28f2d403a3a8f0845eae9861", "repo_url": "https://github.com/pexpect/pexpect", "snippet_lines": ["        first_match = None\n", "\n", "        # 'freshlen' helps a lot here. Further optimizations could\n", "        # possibly include:\n", "        #\n", "        # using something like the Boyer-Moore Fast String Searching\n", "        # Algorithm; pre-compiling the search through a list of\n", "        # strings into something that can scan the input once to\n", "        # search for all N strings; realize that if we search for\n", "        # ['bar', 'baz'] and the input is '...foo' we need not bother\n", "        # rescanning until we've read three more bytes.\n", "        #\n", "        # Sadly, I don't know enough about this interesting topic. /grahn\n", "\n", "        for index, s in self._strings:\n", "            if searchwindowsize is None:\n", "                # the match, if any, can only be in the fresh data,\n", "                # or at the very end of the old data\n", "                offset = -(freshlen + len(s))\n", "            else:\n", "                # better obey searchwindowsize\n", "                offset = -searchwindowsize\n", "            n = buffer.find(s, offset)\n", "            if n >= 0 and (first_match is None or n < first_match):\n", "                first_match = n\n", "                best_index, best_match = index, s\n", "        if first_match is None:\n", "            return -1\n", "        self.match = best_match\n", "        self.start = first_match\n", "        self.end = self.start + len(self.match)\n", "        return best_index\n"], "start_line": 174}, "3:50": {"description": "        Clear the query cache.\n\n        A simple helper that clears the internal query cache.\n", "documentation": {"hmac.new": "Create a new hashing object and return it.\n\n    key: The starting key for the hash.\n    msg: if available, will immediately be hashed into the object's starting\n    state.\n\n    You can now feed arbitrary strings into the object using its update()\n    method, and can ask for the hash value at any time by calling its digest()\n    method.\n    "}, "filename": "cartridge/shop/utils.py", "related_lines": {"4,39": []}, "commit_hash": "244bec069a12efc2ccf2efad6056a1b9323c39e3", "repo_url": "https://github.com/stephenmcd/cartridge", "snippet_lines": ["    key = bytes(settings.SECRET_KEY, encoding=\"utf8\")\n", "    value = bytes(value, encoding=\"utf8\")\n", "    return hmac.new(key, value, digest).hexdigest()\n"], "start_line": 94}, "41:13": {"description": "        Get the object wrapped by ``func``.\n\n\n\n        Follows the chain of :attr:`__wrapped__` attributes returning the last\n\n        object in the chain.\n", "documentation": {"ttype": "The type of the token."}, "filename": "sqlparse/sql.py", "related_lines": {"11,62": []}, "commit_hash": "b7e6ce7b7f687be884eea65df0e576c15b0331dc", "repo_url": "https://github.com/andialbrecht/sqlparse", "snippet_lines": ["        return self.ttype in T.Whitespace\n"], "start_line": 102}, "39:47": {"description": "    \"\"\"Returns all enabled rules.\n\n    :rtype: [Rule]\n", "documentation": {"docs": "An iterable of document dictionaries."}, "filename": "solr/core.py", "related_lines": {"8,48": []}, "commit_hash": "ae72fb90b100090328aab1949ba978b1717795f5", "repo_url": "https://github.com/edsu/solrpy", "snippet_lines": ["        lst = [u'<add>']\n", "        for doc in docs:\n", "            self.__add(lst, doc)\n", "        lst.append(u'</add>')\n", "        return ''.join(lst)\n"], "start_line": 509}, "14:22": {"description": "            '''od.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n            If key is not found, d is returned if given, otherwise KeyError is raised.\n", "documentation": {"requests.get": "Sends a GET request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param params: (optional) Dictionary or bytes to be sent in the query string for the :class:`Request`.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    ", "mimetypes.guess_type": "Guess the type of a file based on its URL.\n\n    Return value is a tuple (type, encoding) where type is None if the\n    type can't be guessed (no or unknown suffix) or a string of the\n    form type/subtype, usable for a MIME Content-type header; and\n    encoding is None for no encoding or the name of the program used\n    to encode (e.g. compress or gzip).  The mappings are table\n    driven.  Encoding suffixes are case sensitive; type suffixes are\n    first tried case sensitive, then case insensitive.\n\n    The suffixes .tgz, .taz and .tz (case sensitive!) are all mapped\n    to \".tar.gz\".  (This is table-driven too, using the dictionary\n    suffix_map).\n\n    Optional `strict' argument when false adds a bunch of commonly found, but\n    non-standard types.\n    "}, "filename": "haul/core.py", "related_lines": {"8,44": []}, "commit_hash": "234024ab8452ea2f41b18561377295cf2879fb20", "repo_url": "https://github.com/vinta/Haul", "snippet_lines": ["        try:\n", "            r = requests.get(url)\n", "        except requests.ConnectionError:\n", "            raise exceptions.RetrieveError('Connection fail')\n", "\n", "        if r.status_code >= 400:\n", "            raise exceptions.RetrieveError('Connected, but status code is %s' % (r.status_code))\n", "\n", "        real_url = r.url\n", "        content = r.content\n", "\n", "        try:\n", "            content_type = r.headers['Content-Type']\n", "        except KeyError:\n", "            content_type, encoding = mimetypes.guess_type(real_url, strict=False)\n", "\n", "        self.response = r\n", "\n", "        return content_type.lower(), content\n"], "start_line": 50}, "14:23": {"description": "    Gets required values for a generator from the field.\n    If required value is a function, calls it with field as argument.\n    If required value is a string, simply fetch the value from the field\n    and return.\n", "documentation": {"requests.get": "Sends a GET request.\n\n    :param url: URL for the new :class:`Request` object.\n    :param params: (optional) Dictionary or bytes to be sent in the query string for the :class:`Request`.\n    :param \\*\\*kwargs: Optional arguments that ``request`` takes.\n    :return: :class:`Response <Response>` object\n    :rtype: requests.Response\n    ", "mimetypes.guess_type": "Guess the type of a file based on its URL.\n\n    Return value is a tuple (type, encoding) where type is None if the\n    type can't be guessed (no or unknown suffix) or a string of the\n    form type/subtype, usable for a MIME Content-type header; and\n    encoding is None for no encoding or the name of the program used\n    to encode (e.g. compress or gzip).  The mappings are table\n    driven.  Encoding suffixes are case sensitive; type suffixes are\n    first tried case sensitive, then case insensitive.\n\n    The suffixes .tgz, .taz and .tz (case sensitive!) are all mapped\n    to \".tar.gz\".  (This is table-driven too, using the dictionary\n    suffix_map).\n\n    Optional `strict' argument when false adds a bunch of commonly found, but\n    non-standard types.\n    "}, "filename": "haul/core.py", "related_lines": {"8,44": []}, "commit_hash": "234024ab8452ea2f41b18561377295cf2879fb20", "repo_url": "https://github.com/vinta/Haul", "snippet_lines": ["        try:\n", "            r = requests.get(url)\n", "        except requests.ConnectionError:\n", "            raise exceptions.RetrieveError('Connection fail')\n", "\n", "        if r.status_code >= 400:\n", "            raise exceptions.RetrieveError('Connected, but status code is %s' % (r.status_code))\n", "\n", "        real_url = r.url\n", "        content = r.content\n", "\n", "        try:\n", "            content_type = r.headers['Content-Type']\n", "        except KeyError:\n", "            content_type, encoding = mimetypes.guess_type(real_url, strict=False)\n", "\n", "        self.response = r\n", "\n", "        return content_type.lower(), content\n"], "start_line": 50}, "36:33": {"description": "    Get pillar cache directory. Initialize it if it does not exist.\n", "documentation": {}, "filename": "schedule/__init__.py", "related_lines": {"11,37": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        del self.jobs[:]\n"], "start_line": 80}, "15:44": {"description": "        \"\"\"Return None.  Labels cells based on `func`.\n        If ``func(cell) is None`` then its datatype is\n        not changed; otherwise it is set to ``func(cell)``.\n", "documentation": {"soup.find_all": "traverses the tree, starting at the given point, and finds all the Tag and NavigableString objects that match the criteria you give"}, "filename": "haul/finders/pipeline/html.py", "related_lines": {"4,43": []}, "commit_hash": "234024ab8452ea2f41b18561377295cf2879fb20", "repo_url": "https://github.com/vinta/Haul", "snippet_lines": ["    now_finder_image_urls = []\n", "\n", "    for img in soup.find_all('img'):\n", "        src = img.get('src', None)\n", "        if src:\n", "            src = str(src)\n", "            if (src not in finder_image_urls) and \\\n", "               (src not in now_finder_image_urls):\n", "                now_finder_image_urls.append(src)\n", "\n", "    output = {}\n", "    output['finder_image_urls'] = finder_image_urls + now_finder_image_urls\n", "\n", "    return output\n"], "start_line": 14}, "15:49": {"description": "        Get access to a specific table.\n\n        Creates a new table, if it hasn't been created before, otherwise it\n        returns the cached :class:`~tinydb.Table` object.\n\n        :param name: The name of the table.\n        :type name: str\n        :param cache_size: How many query results to cache.\n", "documentation": {"soup.find_all": "traverses the tree, starting at the given point, and finds all the Tag and NavigableString objects that match the criteria you give"}, "filename": "haul/finders/pipeline/html.py", "related_lines": {"4,43": []}, "commit_hash": "234024ab8452ea2f41b18561377295cf2879fb20", "repo_url": "https://github.com/vinta/Haul", "snippet_lines": ["    now_finder_image_urls = []\n", "\n", "    for img in soup.find_all('img'):\n", "        src = img.get('src', None)\n", "        if src:\n", "            src = str(src)\n", "            if (src not in finder_image_urls) and \\\n", "               (src not in now_finder_image_urls):\n", "                now_finder_image_urls.append(src)\n", "\n", "    output = {}\n", "    output['finder_image_urls'] = finder_image_urls + now_finder_image_urls\n", "\n", "    return output\n"], "start_line": 14}, "8:23": {"description": "    Gets required values for a generator from the field.\n    If required value is a function, calls it with field as argument.\n    If required value is a string, simply fetch the value from the field\n    and return.\n", "documentation": {"disable": "\n    Disable a service.\n\n    ::\n\n        fabtools.systemd.disable('httpd')\n\n    .. note:: This function is idempotent.\n    ", "stop": "\n    Stop a service.\n\n    ::\n\n        if fabtools.systemd.is_running('foo'):\n            fabtools.systemd.stop('foo')\n\n    .. note:: This function is idempotent.\n    "}, "filename": "fabtools/systemd.py", "related_lines": {"4,30": []}, "commit_hash": "f7e851efaab057becce4ce0b8bd5fba146bf34c3", "repo_url": "https://github.com/ronnix/fabtools", "snippet_lines": ["    stop(service)\n", "    disable(service)\n"], "start_line": 132}, "6:40": {"description": "        \"\"\"Set the filter attributes to its default values\"\"\"\n", "documentation": {}, "filename": "diesel/pipeline.py", "related_lines": {"11,50": []}, "commit_hash": "8d48371fce0b79d6631053594bce06e4b9628499", "repo_url": "https://github.com/dieseldev/diesel", "snippet_lines": ["        if not self.current and not self.line:\n", "            if self.want_close:\n", "                raise PipelineCloseRequest()\n", "            return ''\n", "\n", "        if not self.current:\n", "            _, self.current = self.line.pop(0)\n", "            self.current.reset()\n", "\n", "        out = ''\n", "        while len(out) < amt:\n", "            try:\n", "                data = self.current.read(amt - len(out))\n", "            except ValueError:\n", "                data = ''\n", "            if data == '':\n", "                if not self.line:\n", "                    self.current = None\n", "                    break\n", "                _, self.current = self.line.pop(0)\n", "                self.current.reset()\n", "            else:\n", "                out += data\n"], "start_line": 108}, "35:6": {"description": "        '''Read up to `amt` bytes off the pipeline.\n\n        May raise PipelineCloseRequest if the pipeline is\n        empty and the connected stream should be closed.\n", "documentation": {"": "logger.info,time.sleep", "logger.info": "\n        Log 'msg % args' with severity 'INFO'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=1)\n        ", "time.sleep": "sleep(seconds)\n\nDelay execution for a given number of seconds.  The argument may be\na floating point number for subsecond precision."}, "filename": "schedule/__init__.py", "related_lines": {"83,135": [], "11,70": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        logger.info('Running *all* %i jobs with %is delay inbetween',\n", "                    len(self.jobs), delay_seconds)\n", "        for job in self.jobs:\n", "            self._run_job(job)\n", "            time.sleep(delay_seconds)\n"], "start_line": 72}, "49:5": {"description": "    # get indices for frequency and time\n", "documentation": {"_read": "Reading access to the DB."}, "filename": "tinydb/database.py", "related_lines": {"104,173": [], "49,102": []}, "commit_hash": "bc2b630dd1dd996a4aa0de07decb4c7ea0190b0e", "repo_url": "https://github.com/msiemens/tinydb", "snippet_lines": ["        if name in self._table_cache:\n", "            return self._table_cache[name]\n", "\n", "        table = self.table_class(StorageProxy(self._storage, name), **options)\n", "\n", "        self._table_cache[name] = table\n", "        \n", "        # table._read will create an empty table in the storage, if necessary\n", "        table._read()\n", "\n", "        return table\n"], "start_line": 107}, "50:5": {"description": "    # get indices for frequency and time\n", "documentation": {}, "filename": "tinydb/database.py", "related_lines": {"8,29": []}, "commit_hash": "bc2b630dd1dd996a4aa0de07decb4c7ea0190b0e", "repo_url": "https://github.com/msiemens/tinydb", "snippet_lines": ["        self._query_cache.clear()\n"], "start_line": 257}, "3:2": {"description": "    Stores the tax type and total in the session.\n", "documentation": {"hmac.new": "Create a new hashing object and return it.\n\n    key: The starting key for the hash.\n    msg: if available, will immediately be hashed into the object's starting\n    state.\n\n    You can now feed arbitrary strings into the object using its update()\n    method, and can ask for the hash value at any time by calling its digest()\n    method.\n    "}, "filename": "cartridge/shop/utils.py", "related_lines": {"4,39": []}, "commit_hash": "244bec069a12efc2ccf2efad6056a1b9323c39e3", "repo_url": "https://github.com/stephenmcd/cartridge", "snippet_lines": ["    key = bytes(settings.SECRET_KEY, encoding=\"utf8\")\n", "    value = bytes(value, encoding=\"utf8\")\n", "    return hmac.new(key, value, digest).hexdigest()\n"], "start_line": 94}, "50:8": {"description": "    Stop and disable a service (convenience function).\n\n    .. note:: This function is idempotent.\n", "documentation": {}, "filename": "tinydb/database.py", "related_lines": {"8,29": []}, "commit_hash": "bc2b630dd1dd996a4aa0de07decb4c7ea0190b0e", "repo_url": "https://github.com/msiemens/tinydb", "snippet_lines": ["        self._query_cache.clear()\n"], "start_line": 257}, "49:11": {"description": "    A functools.partial alternative, which returns a real function.\n\n    Can be used to construct methods.\n", "documentation": {"_read": "Reading access to the DB."}, "filename": "tinydb/database.py", "related_lines": {"104,173": [], "49,102": []}, "commit_hash": "bc2b630dd1dd996a4aa0de07decb4c7ea0190b0e", "repo_url": "https://github.com/msiemens/tinydb", "snippet_lines": ["        if name in self._table_cache:\n", "            return self._table_cache[name]\n", "\n", "        table = self.table_class(StorageProxy(self._storage, name), **options)\n", "\n", "        self._table_cache[name] = table\n", "        \n", "        # table._read will create an empty table in the storage, if necessary\n", "        table._read()\n", "\n", "        return table\n"], "start_line": 107}, "21:29": {"description": "        '''Wait for the subprocess to finish.\n        \n        Returns the exit code.\n", "documentation": {}, "filename": "micawber/compat.py", "related_lines": {"39,76": []}, "commit_hash": "749f32825eedcdea69be9eb874d9ded0bd227dc3", "repo_url": "https://github.com/coleifer/micawber", "snippet_lines": ["            if not self:\n", "                raise KeyError('dictionary is empty')\n", "            root = self.__root\n", "            if last:\n", "                link = root[0]\n", "                link_prev = link[0]\n", "                link_prev[1] = root\n", "                root[0] = link_prev\n", "            else:\n", "                link = root[1]\n", "                link_next = link[1]\n", "                root[1] = link_next\n", "                link_next[0] = root\n", "            key = link[2]\n", "            del self.__map[key]\n", "            value = dict.pop(self, key)\n", "            return key, value\n"], "start_line": 108}, "22:33": {"description": "    Get pillar cache directory. Initialize it if it does not exist.\n", "documentation": {}, "filename": "micawber/compat.py", "related_lines": {"60,90": [], "149,177": [], "104,147": [], "35,55": []}, "commit_hash": "749f32825eedcdea69be9eb874d9ded0bd227dc3", "repo_url": "https://github.com/coleifer/micawber", "snippet_lines": ["            if key in self:\n", "                result = self[key]\n", "                del self[key]\n", "                return result\n", "            if default is self.__marker:\n", "                raise KeyError(key)\n", "            return default\n"], "start_line": 194}, "2:50": {"description": "        Clear the query cache.\n\n        A simple helper that clears the internal query cache.\n", "documentation": {"_str": "\n    A backport of the Python 3 str object to Py2\n    "}, "filename": "cartridge/shop/utils.py", "related_lines": {"4,48": []}, "commit_hash": "244bec069a12efc2ccf2efad6056a1b9323c39e3", "repo_url": "https://github.com/stephenmcd/cartridge", "snippet_lines": ["    request.session[\"tax_type\"] = _str(tax_type)\n", "    request.session[\"tax_total\"] = _str(tax_total)\n"], "start_line": 85}, "21:20": {"description": "        \"\"\"Loads the encoded object.  This function raises :class:`BadPayload`\n        if the payload is not valid.  The `serializer` parameter can be used to\n        override the serializer stored on the class.  The encoded payload is\n        always byte based.\n", "documentation": {}, "filename": "micawber/compat.py", "related_lines": {"39,76": []}, "commit_hash": "749f32825eedcdea69be9eb874d9ded0bd227dc3", "repo_url": "https://github.com/coleifer/micawber", "snippet_lines": ["            if not self:\n", "                raise KeyError('dictionary is empty')\n", "            root = self.__root\n", "            if last:\n", "                link = root[0]\n", "                link_prev = link[0]\n", "                link_prev[1] = root\n", "                root[0] = link_prev\n", "            else:\n", "                link = root[1]\n", "                link_next = link[1]\n", "                root[1] = link_next\n", "                link_next[0] = root\n", "            key = link[2]\n", "            del self.__map[key]\n", "            value = dict.pop(self, key)\n", "            return key, value\n"], "start_line": 108}, "3:49": {"description": "        Get access to a specific table.\n\n        Creates a new table, if it hasn't been created before, otherwise it\n        returns the cached :class:`~tinydb.Table` object.\n\n        :param name: The name of the table.\n        :type name: str\n        :param cache_size: How many query results to cache.\n", "documentation": {"hmac.new": "Create a new hashing object and return it.\n\n    key: The starting key for the hash.\n    msg: if available, will immediately be hashed into the object's starting\n    state.\n\n    You can now feed arbitrary strings into the object using its update()\n    method, and can ask for the hash value at any time by calling its digest()\n    method.\n    "}, "filename": "cartridge/shop/utils.py", "related_lines": {"4,39": []}, "commit_hash": "244bec069a12efc2ccf2efad6056a1b9323c39e3", "repo_url": "https://github.com/stephenmcd/cartridge", "snippet_lines": ["    key = bytes(settings.SECRET_KEY, encoding=\"utf8\")\n", "    value = bytes(value, encoding=\"utf8\")\n", "    return hmac.new(key, value, digest).hexdigest()\n"], "start_line": 94}, "1:16": {"description": "    \"\"\"Parse a string or file-like object into a tree\"\"\"\n", "documentation": {"_str": "\n    A backport of the Python 3 str object to Py2\n    "}, "filename": "cartridge/shop/utils.py", "related_lines": {"4,53": []}, "commit_hash": "244bec069a12efc2ccf2efad6056a1b9323c39e3", "repo_url": "https://github.com/stephenmcd/cartridge", "snippet_lines": ["    request.session[\"shipping_type\"] = _str(shipping_type)\n", "    request.session[\"shipping_total\"] = _str(shipping_total)\n"], "start_line": 77}, "7:0": {"description": "    Removes values for the given session variables names\n    if they exist.\n", "documentation": {}, "filename": "fabtools/systemd.py", "related_lines": {"4,20": []}, "commit_hash": "f7e851efaab057becce4ce0b8bd5fba146bf34c3", "repo_url": "https://github.com/ronnix/fabtools", "snippet_lines": ["    action('enable', service)\n"], "start_line": 30}, "34:41": {"description": "        \"\"\"Return ``True`` if this token is a whitespace token.\"\"\"\n", "documentation": {"should_run": "True if the job should be run now", "job.should_run": "true if the job should be run now", "sorted": "Return a new sorted list from the items in iterable."}, "filename": "schedule/__init__.py", "related_lines": {"11,49": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        runnable_jobs = (job for job in self.jobs if job.should_run)\n", "        for job in sorted(runnable_jobs):\n", "            self._run_job(job)\n"], "start_line": 62}, "20:44": {"description": "        \"\"\"Return None.  Labels cells based on `func`.\n        If ``func(cell) is None`` then its datatype is\n        not changed; otherwise it is set to ``func(cell)``.\n", "documentation": {"serializer.loads": "Like :meth:`dumps` but dumps into a file. The file handle has to be compatible with what the internal serializer expects.", "is_text_serializer": "Checks whether a serializer generates text or binary."}, "filename": "itsdangerous.py", "related_lines": {"38,114": [], "11,35": []}, "commit_hash": "0f6d5c5992cf9c3e857474ca6d7ccfa604df60da", "repo_url": "https://github.com/pallets/itsdangerous", "snippet_lines": ["        if serializer is None:\n", "            serializer = self.serializer\n", "            is_text = self.is_text_serializer\n", "        else:\n", "            is_text = is_text_serializer(serializer)\n", "        try:\n", "            if is_text:\n", "                payload = payload.decode('utf-8')\n", "            return serializer.loads(payload)\n", "        except Exception as e:\n", "            raise BadPayload('Could not load the payload because an '\n", "                'exception occurred on unserializing the data',\n", "                original_error=e)\n"], "start_line": 533}, "18:34": {"description": "        \"\"\"Run all jobs that are scheduled to run.\n\n\n\n        Please note that it is *intended behavior that tick() does not\n\n        run missed jobs*. For example, if you've registered a job that\n\n        should run every minute and you only call tick() in one hour\n\n        increments then your job won't be run 60 times in between but\n\n        only once.\n", "documentation": {}, "filename": "html5lib/_inputstream.py", "related_lines": {"11,64": [], "113,169": []}, "commit_hash": "a3022dcea691780d300547bbf68b4dd921995d1c", "repo_url": "https://github.com/html5lib/html5lib-python", "snippet_lines": ["        p = self.position\n", "        data = self[p:p + len(bytes)]\n", "        rv = data.startswith(bytes)\n", "        if rv:\n", "            self.position += len(bytes)\n", "        return rv\n"], "start_line": 667}, "6:3": {"description": "    Returns the hash of the given value, used for signing order key stored in\n    cookie for remembering address fields.\n", "documentation": {}, "filename": "diesel/pipeline.py", "related_lines": {"11,50": []}, "commit_hash": "8d48371fce0b79d6631053594bce06e4b9628499", "repo_url": "https://github.com/dieseldev/diesel", "snippet_lines": ["        if not self.current and not self.line:\n", "            if self.want_close:\n", "                raise PipelineCloseRequest()\n", "            return ''\n", "\n", "        if not self.current:\n", "            _, self.current = self.line.pop(0)\n", "            self.current.reset()\n", "\n", "        out = ''\n", "        while len(out) < amt:\n", "            try:\n", "                data = self.current.read(amt - len(out))\n", "            except ValueError:\n", "                data = ''\n", "            if data == '':\n", "                if not self.line:\n", "                    self.current = None\n", "                    break\n", "                _, self.current = self.line.pop(0)\n", "                self.current.reset()\n", "            else:\n", "                out += data\n"], "start_line": 108}, "20:9": {"description": "    Consume an iterable not reading it into memory; return the number of items.\n", "documentation": {"serializer.loads": "Like :meth:`dumps` but dumps into a file. The file handle has to be compatible with what the internal serializer expects.", "is_text_serializer": "Checks whether a serializer generates text or binary."}, "filename": "itsdangerous.py", "related_lines": {"38,114": [], "11,35": []}, "commit_hash": "0f6d5c5992cf9c3e857474ca6d7ccfa604df60da", "repo_url": "https://github.com/pallets/itsdangerous", "snippet_lines": ["        if serializer is None:\n", "            serializer = self.serializer\n", "            is_text = self.is_text_serializer\n", "        else:\n", "            is_text = is_text_serializer(serializer)\n", "        try:\n", "            if is_text:\n", "                payload = payload.decode('utf-8')\n", "            return serializer.loads(payload)\n", "        except Exception as e:\n", "            raise BadPayload('Could not load the payload because an '\n", "                'exception occurred on unserializing the data',\n", "                original_error=e)\n"], "start_line": 533}, "45:9": {"description": "    Consume an iterable not reading it into memory; return the number of items.\n", "documentation": {"get_file_obj": "\n    Light wrapper to handle strings and let files (anything else) pass through.\n\n    It also handle '.gz' files.\n\n    Parameters\n    ==========\n    fname: string or file-like object\n        File to open / forward\n    mode: string\n        Argument passed to the 'open' or 'gzip.open' function\n    encoding: string\n        For Python 3 only, specify the encoding of the file\n\n    Returns\n    =======\n    A file-like object that is always a context-manager. If the `fname` was already a file-like object,\n    the returned context manager *will not close the file*.\n    ", "cPickle.load": "load(file) -- Load a pickle from the given file"}, "filename": "statsmodels/iolib/smpickle.py", "related_lines": {"4,44": []}, "commit_hash": "036ad9f769068a50d48048c889762a1e32e5eb1a", "repo_url": "https://github.com/statsmodels/statsmodels", "snippet_lines": ["    with get_file_obj(fname, 'rb') as fin:\n", "        return cPickle.load(fin)\n"], "start_line": 31}, "42:13": {"description": "        Get the object wrapped by ``func``.\n\n\n\n        Follows the chain of :attr:`__wrapped__` attributes returning the last\n\n        object in the chain.\n", "documentation": {}, "filename": "sqlparse/sql.py", "related_lines": {"11,70": []}, "commit_hash": "b7e6ce7b7f687be884eea65df0e576c15b0331dc", "repo_url": "https://github.com/andialbrecht/sqlparse", "snippet_lines": ["        return self.parent == other\n"], "start_line": 119}, "19:27": {"description": "        '''This searches 'buffer' for the first occurence of one of the search\n        strings.  'freshlen' must indicate the number of bytes at the end of\n        'buffer' which have not been searched before. It helps to avoid\n        searching the same, possibly big, buffer over and over again.\n\n        See class spawn for the 'searchwindowsize' argument.\n\n        If there is a match this returns the index of that string, and sets\n        'start', 'end' and 'match'. Otherwise, this returns -1. '''\n", "documentation": {}, "filename": "html5lib/_inputstream.py", "related_lines": {"11,72": [], "74,152": []}, "commit_hash": "a3022dcea691780d300547bbf68b4dd921995d1c", "repo_url": "https://github.com/html5lib/html5lib-python", "snippet_lines": ["        newPosition = self[self.position:].find(bytes)\n", "        if newPosition > -1:\n", "            # XXX: This is ugly, but I can't see a nicer way to fix this.\n", "            if self._position == -1:\n", "                self._position = 0\n", "            self._position += (newPosition + len(bytes) - 1)\n", "            return True\n", "        else:\n", "            raise StopIteration\n"], "start_line": 677}, "16:18": {"description": "        \"\"\"Look for a sequence of bytes at the start of a string. If the bytes\n        are found return True and advance the position to the byte after the\n        match. Otherwise return False and leave the position alone\"\"\"\n", "documentation": {"treebuilders.getTreeBuilder": "Get a TreeBuilder class for various types of tree with built-in support\n\n    treeType - the name of the tree type required (case-insensitive). Supported\n               values are:\n\n               \"dom\" - A generic builder for DOM implementations, defaulting to\n                       a xml.dom.minidom based implementation.\n               \"etree\" - A generic builder for tree implementations exposing an\n                         ElementTree-like interface, defaulting to\n                         xml.etree.cElementTree if available and\n                         xml.etree.ElementTree if not.\n               \"lxml\" - A etree-based builder for lxml.etree, handling\n                        limitations of lxml's implementation.\n\n    implementation - (Currently applies to the \"etree\" and \"dom\" tree types). A\n                      module implementing the tree type e.g.\n                      xml.etree.ElementTree or xml.etree.cElementTree.", "parse": "Parse a HTML document into a well-formed tree", "HTMLParser": "HTML parser. Generates a tree structure from a stream of (possibly\n        malformed) HTML"}, "filename": "html5lib/html5parser.py", "related_lines": {"7,53": []}, "commit_hash": "a3022dcea691780d300547bbf68b4dd921995d1c", "repo_url": "https://github.com/html5lib/html5lib-python", "snippet_lines": ["    tb = treebuilders.getTreeBuilder(treebuilder)\n", "    p = HTMLParser(tb, namespaceHTMLElements=namespaceHTMLElements)\n", "    return p.parse(doc, **kwargs)\n"], "start_line": 33}, "25:45": {"description": "    Load a previously saved object from file\n\n    Parameters\n    ----------\n    fname : str\n        Filename to unpickle\n\n    Notes\n    -----\n    This method can be used to load *both* models and results.\n", "documentation": {}, "filename": "mycli/clitoolbar.py", "related_lines": {"4,55": []}, "commit_hash": "a69bacce987dc8e4c5a5de71b882dbbf8949e916", "repo_url": "https://github.com/dbcli/mycli", "snippet_lines": ["    token = Token.Toolbar\n", "\n", "    def get_toolbar_tokens(cli):\n", "        result = []\n", "        result.append((token, ' '))\n", "\n", "        if cli.buffers[DEFAULT_BUFFER].completer.smart_completion:\n", "            result.append((token.On, '[F2] Smart Completion: ON  '))\n", "        else:\n", "            result.append((token.Off, '[F2] Smart Completion: OFF  '))\n", "\n", "        if cli.buffers[DEFAULT_BUFFER].always_multiline:\n", "            result.append((token.On, '[F3] Multiline: ON  '))\n", "        else:\n", "            result.append((token.Off, '[F3] Multiline: OFF  '))\n", "\n", "        if cli.buffers[DEFAULT_BUFFER].always_multiline:\n", "            result.append((token,\n", "                ' (Semi-colon [;] will end the line)'))\n", "\n", "        if cli.editing_mode == EditingMode.VI:\n", "            result.append((token.On, '[F4] Vi-mode'))\n", "        else:\n", "            result.append((token.On, '[F4] Emacs-mode'))\n", "\n", "        if get_is_refreshing():\n", "            result.append((token, '     Refreshing completions...'))\n", "\n", "        return result\n", "    return get_toolbar_tokens\n"], "start_line": 8}, "36:48": {"description": "    \"\"\"Prints alias for current shell.\"\"\"\n", "documentation": {}, "filename": "schedule/__init__.py", "related_lines": {"11,37": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        del self.jobs[:]\n"], "start_line": 80}, "44:26": {"description": "    \"\"\"Read and merge a list of config files.\"\"\"\n", "documentation": {}, "filename": "statsmodels/iolib/table.py", "related_lines": {"25,53": [], "131,168": []}, "commit_hash": "036ad9f769068a50d48048c889762a1e32e5eb1a", "repo_url": "https://github.com/statsmodels/statsmodels", "snippet_lines": ["        for row in self:\n", "            for cell in row:\n", "                label = func(cell)\n", "                if label is not None:\n", "                    cell.datatype = label\n"], "start_line": 475}, "16:15": {"description": "    Find image URL in <img>'s src attribute\n", "documentation": {"treebuilders.getTreeBuilder": "Get a TreeBuilder class for various types of tree with built-in support\n\n    treeType - the name of the tree type required (case-insensitive). Supported\n               values are:\n\n               \"dom\" - A generic builder for DOM implementations, defaulting to\n                       a xml.dom.minidom based implementation.\n               \"etree\" - A generic builder for tree implementations exposing an\n                         ElementTree-like interface, defaulting to\n                         xml.etree.cElementTree if available and\n                         xml.etree.ElementTree if not.\n               \"lxml\" - A etree-based builder for lxml.etree, handling\n                        limitations of lxml's implementation.\n\n    implementation - (Currently applies to the \"etree\" and \"dom\" tree types). A\n                      module implementing the tree type e.g.\n                      xml.etree.ElementTree or xml.etree.cElementTree.", "parse": "Parse a HTML document into a well-formed tree", "HTMLParser": "HTML parser. Generates a tree structure from a stream of (possibly\n        malformed) HTML"}, "filename": "html5lib/html5parser.py", "related_lines": {"7,53": []}, "commit_hash": "a3022dcea691780d300547bbf68b4dd921995d1c", "repo_url": "https://github.com/html5lib/html5lib-python", "snippet_lines": ["    tb = treebuilders.getTreeBuilder(treebuilder)\n", "    p = HTMLParser(tb, namespaceHTMLElements=namespaceHTMLElements)\n", "    return p.parse(doc, **kwargs)\n"], "start_line": 33}, "24:7": {"description": "    Enable a service.\n\n    ::\n\n        fabtools.enable('httpd')\n\n    .. note:: This function is idempotent.\n", "documentation": {"import_from_str": "\n    Import and return an object defined as import string in the form of\n\n        path.to.module.object_name\n    "}, "filename": "model_mommy/utils.py", "related_lines": {"116,159": [], "4,56": []}, "commit_hash": "c7aa8d923f842d6961d8f56a6f34635c68352e87", "repo_url": "https://github.com/vandersonmota/model_mommy", "snippet_lines": ["    if isinstance(import_string_or_obj, string_types):\n", "        return import_from_str(import_string_or_obj)\n", "    return import_string_or_obj\n"], "start_line": 19}, "16:50": {"description": "        Clear the query cache.\n\n        A simple helper that clears the internal query cache.\n", "documentation": {"treebuilders.getTreeBuilder": "Get a TreeBuilder class for various types of tree with built-in support\n\n    treeType - the name of the tree type required (case-insensitive). Supported\n               values are:\n\n               \"dom\" - A generic builder for DOM implementations, defaulting to\n                       a xml.dom.minidom based implementation.\n               \"etree\" - A generic builder for tree implementations exposing an\n                         ElementTree-like interface, defaulting to\n                         xml.etree.cElementTree if available and\n                         xml.etree.ElementTree if not.\n               \"lxml\" - A etree-based builder for lxml.etree, handling\n                        limitations of lxml's implementation.\n\n    implementation - (Currently applies to the \"etree\" and \"dom\" tree types). A\n                      module implementing the tree type e.g.\n                      xml.etree.ElementTree or xml.etree.cElementTree.", "parse": "Parse a HTML document into a well-formed tree", "HTMLParser": "HTML parser. Generates a tree structure from a stream of (possibly\n        malformed) HTML"}, "filename": "html5lib/html5parser.py", "related_lines": {"7,53": []}, "commit_hash": "a3022dcea691780d300547bbf68b4dd921995d1c", "repo_url": "https://github.com/html5lib/html5lib-python", "snippet_lines": ["    tb = treebuilders.getTreeBuilder(treebuilder)\n", "    p = HTMLParser(tb, namespaceHTMLElements=namespaceHTMLElements)\n", "    return p.parse(doc, **kwargs)\n"], "start_line": 33}, "44:51": {"description": "        Push a key to the tail of the LRU queue\n", "documentation": {}, "filename": "statsmodels/iolib/table.py", "related_lines": {"25,53": [], "131,168": []}, "commit_hash": "036ad9f769068a50d48048c889762a1e32e5eb1a", "repo_url": "https://github.com/statsmodels/statsmodels", "snippet_lines": ["        for row in self:\n", "            for cell in row:\n", "                label = func(cell)\n", "                if label is not None:\n", "                    cell.datatype = label\n"], "start_line": 475}, "2:7": {"description": "    Enable a service.\n\n    ::\n\n        fabtools.enable('httpd')\n\n    .. note:: This function is idempotent.\n", "documentation": {"_str": "\n    A backport of the Python 3 str object to Py2\n    "}, "filename": "cartridge/shop/utils.py", "related_lines": {"4,48": []}, "commit_hash": "244bec069a12efc2ccf2efad6056a1b9323c39e3", "repo_url": "https://github.com/stephenmcd/cartridge", "snippet_lines": ["    request.session[\"tax_type\"] = _str(tax_type)\n", "    request.session[\"tax_total\"] = _str(tax_total)\n"], "start_line": 85}, "26:8": {"description": "    Stop and disable a service (convenience function).\n\n    .. note:: This function is idempotent.\n", "documentation": {"ConfigObj": "An object to read, create, and write config files.", "merge": "This method is a recursive update method. It allows you to merge two config files together.", "read_config_file": "Read a config file."}, "filename": "mycli/config.py", "related_lines": {"7,44": []}, "commit_hash": "a69bacce987dc8e4c5a5de71b882dbbf8949e916", "repo_url": "https://github.com/dbcli/mycli", "snippet_lines": ["    config = ConfigObj()\n", "\n", "    for _file in files:\n", "        _config = read_config_file(_file)\n", "        if bool(_config) is True:\n", "            config.merge(_config)\n", "            config.filename = _config.filename\n", "\n", "    return config\n"], "start_line": 59}, "22:2": {"description": "    Stores the tax type and total in the session.\n", "documentation": {}, "filename": "micawber/compat.py", "related_lines": {"60,90": [], "149,177": [], "104,147": [], "35,55": []}, "commit_hash": "749f32825eedcdea69be9eb874d9ded0bd227dc3", "repo_url": "https://github.com/coleifer/micawber", "snippet_lines": ["            if key in self:\n", "                result = self[key]\n", "                del self[key]\n", "                return result\n", "            if default is self.__marker:\n", "                raise KeyError(key)\n", "            return default\n"], "start_line": 194}, "4:17": {"description": "        \"\"\"Attempts to detect at BOM at the start of the stream. If\n        an encoding can be determined from the BOM return the name of the\n        encoding otherwise return None\"\"\"\n", "documentation": {}, "filename": "dejavu/fingerprint.py", "related_lines": {"6,19": []}, "commit_hash": "4ab26bd0dbca3f349f1c44a308f204c6b1cb2d8a", "repo_url": "https://github.com/worldveil/dejavu", "snippet_lines": ["    return generate_hashes(local_maxima, fan_value=fan_value)\n"], "start_line": 89}, "4:16": {"description": "    \"\"\"Parse a string or file-like object into a tree\"\"\"\n", "documentation": {}, "filename": "dejavu/fingerprint.py", "related_lines": {"6,19": []}, "commit_hash": "4ab26bd0dbca3f349f1c44a308f204c6b1cb2d8a", "repo_url": "https://github.com/worldveil/dejavu", "snippet_lines": ["    return generate_hashes(local_maxima, fan_value=fan_value)\n"], "start_line": 89}, "20:14": {"description": "        Use requests to fetch remote content\n", "documentation": {"serializer.loads": "Like :meth:`dumps` but dumps into a file. The file handle has to be compatible with what the internal serializer expects.", "is_text_serializer": "Checks whether a serializer generates text or binary."}, "filename": "itsdangerous.py", "related_lines": {"38,114": [], "11,35": []}, "commit_hash": "0f6d5c5992cf9c3e857474ca6d7ccfa604df60da", "repo_url": "https://github.com/pallets/itsdangerous", "snippet_lines": ["        if serializer is None:\n", "            serializer = self.serializer\n", "            is_text = self.is_text_serializer\n", "        else:\n", "            is_text = is_text_serializer(serializer)\n", "        try:\n", "            if is_text:\n", "                payload = payload.decode('utf-8')\n", "            return serializer.loads(payload)\n", "        except Exception as e:\n", "            raise BadPayload('Could not load the payload because an '\n", "                'exception occurred on unserializing the data',\n", "                original_error=e)\n"], "start_line": 533}, "22:8": {"description": "    Stop and disable a service (convenience function).\n\n    .. note:: This function is idempotent.\n", "documentation": {}, "filename": "micawber/compat.py", "related_lines": {"60,90": [], "149,177": [], "104,147": [], "35,55": []}, "commit_hash": "749f32825eedcdea69be9eb874d9ded0bd227dc3", "repo_url": "https://github.com/coleifer/micawber", "snippet_lines": ["            if key in self:\n", "                result = self[key]\n", "                del self[key]\n", "                return result\n", "            if default is self.__marker:\n", "                raise KeyError(key)\n", "            return default\n"], "start_line": 194}, "13:20": {"description": "        \"\"\"Loads the encoded object.  This function raises :class:`BadPayload`\n        if the payload is not valid.  The `serializer` parameter can be used to\n        override the serializer stored on the class.  The encoded payload is\n        always byte based.\n", "documentation": {}, "filename": "funcy/decorators.py", "related_lines": {"55,155": [], "8,42": []}, "commit_hash": "00863de7a8c5be2ac6503493e01f0b0281bab128", "repo_url": "https://github.com/Suor/funcy", "snippet_lines": ["        f = func  # remember the original func for error reporting\n", "        memo = set([id(f)]) # Memoise by id to tolerate non-hashable objects\n", "        while hasattr(func, '__wrapped__'):\n", "            func = func.__wrapped__\n", "            id_func = id(func)\n", "            if id_func in memo:\n", "                raise ValueError('wrapper loop when unwrapping {!r}'.format(f))\n", "            memo.add(id_func)\n", "        return func\n"], "start_line": 276}, "34:4": {"description": "    # return hashes\n", "documentation": {"should_run": "True if the job should be run now", "job.should_run": "true if the job should be run now", "sorted": "Return a new sorted list from the items in iterable."}, "filename": "schedule/__init__.py", "related_lines": {"11,49": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        runnable_jobs = (job for job in self.jobs if job.should_run)\n", "        for job in sorted(runnable_jobs):\n", "            self._run_job(job)\n"], "start_line": 62}, "19:32": {"description": "        Return the value at key ``name``, raises a KeyError if the key\n        doesn't exist.\n", "documentation": {}, "filename": "html5lib/_inputstream.py", "related_lines": {"11,72": [], "74,152": []}, "commit_hash": "a3022dcea691780d300547bbf68b4dd921995d1c", "repo_url": "https://github.com/html5lib/html5lib-python", "snippet_lines": ["        newPosition = self[self.position:].find(bytes)\n", "        if newPosition > -1:\n", "            # XXX: This is ugly, but I can't see a nicer way to fix this.\n", "            if self._position == -1:\n", "                self._position = 0\n", "            self._position += (newPosition + len(bytes) - 1)\n", "            return True\n", "        else:\n", "            raise StopIteration\n"], "start_line": 677}, "23:5": {"description": "    # get indices for frequency and time\n", "documentation": {}, "filename": "model_mommy/mommy.py", "related_lines": {"131,199": [], "61,125": []}, "commit_hash": "c7aa8d923f842d6961d8f56a6f34635c68352e87", "repo_url": "https://github.com/vandersonmota/model_mommy", "snippet_lines": ["    rt = {}\n", "    if hasattr(generator, 'required'):\n", "        for item in generator.required:\n", "\n", "            if callable(item):  # mommy can deal with the nasty hacking too!\n", "                key, value = item(field)\n", "                rt[key] = value\n", "\n", "            elif isinstance(item, string_types):\n", "                rt[item] = getattr(field, item)\n", "\n", "            else:\n", "                raise ValueError(\"Required value '%s' is of wrong type. \\\n", "                                  Don't make mommy sad.\" % str(item))\n", "\n", "    return rt\n"], "start_line": 471}, "36:1": {"description": "    Stores the shipping type and total in the session.\n", "documentation": {}, "filename": "schedule/__init__.py", "related_lines": {"11,37": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        del self.jobs[:]\n"], "start_line": 80}, "30:42": {"description": "        \"\"\"Returns ``True`` if this token is a direct child of *other*.\"\"\"\n", "documentation": {}, "filename": "pexpect/popen_spawn.py", "related_lines": {"11,32": []}, "commit_hash": "cb2f06f07d5a8dcd28f2d403a3a8f0845eae9861", "repo_url": "https://github.com/pexpect/pexpect", "snippet_lines": ["        self.proc.stdin.close()\n"], "start_line": 179}, "11:19": {"description": "        \"\"\"Look for the next sequence of bytes matching a given sequence. If\n        a match is found advance the position to the last byte of the match\"\"\"\n", "documentation": {}, "filename": "funcy/simple_funcs.py", "related_lines": {"37,66": []}, "commit_hash": "00863de7a8c5be2ac6503493e01f0b0281bab128", "repo_url": "https://github.com/Suor/funcy", "snippet_lines": ["    return lambda *a, **kw: func(*(args + a), **dict(kwargs, **kw))\n"], "start_line": 28}, "27:46": {"description": "    Save the object to file via pickling.\n\n    Parameters\n    ----------\n    fname : str\n        Filename to pickle to\n", "documentation": {}, "filename": "pexpect/expect.py", "related_lines": {"432,471": [], "369,426": [], "473,499": []}, "commit_hash": "cb2f06f07d5a8dcd28f2d403a3a8f0845eae9861", "repo_url": "https://github.com/pexpect/pexpect", "snippet_lines": ["        first_match = None\n", "\n", "        # 'freshlen' helps a lot here. Further optimizations could\n", "        # possibly include:\n", "        #\n", "        # using something like the Boyer-Moore Fast String Searching\n", "        # Algorithm; pre-compiling the search through a list of\n", "        # strings into something that can scan the input once to\n", "        # search for all N strings; realize that if we search for\n", "        # ['bar', 'baz'] and the input is '...foo' we need not bother\n", "        # rescanning until we've read three more bytes.\n", "        #\n", "        # Sadly, I don't know enough about this interesting topic. /grahn\n", "\n", "        for index, s in self._strings:\n", "            if searchwindowsize is None:\n", "                # the match, if any, can only be in the fresh data,\n", "                # or at the very end of the old data\n", "                offset = -(freshlen + len(s))\n", "            else:\n", "                # better obey searchwindowsize\n", "                offset = -searchwindowsize\n", "            n = buffer.find(s, offset)\n", "            if n >= 0 and (first_match is None or n < first_match):\n", "                first_match = n\n", "                best_index, best_match = index, s\n", "        if first_match is None:\n", "            return -1\n", "        self.match = best_match\n", "        self.start = first_match\n", "        self.end = self.start + len(self.match)\n", "        return best_index\n"], "start_line": 174}, "36:7": {"description": "    Enable a service.\n\n    ::\n\n        fabtools.enable('httpd')\n\n    .. note:: This function is idempotent.\n", "documentation": {}, "filename": "schedule/__init__.py", "related_lines": {"11,37": []}, "commit_hash": "9a37002ef7721138be4ce13dd94a87a438345b86", "repo_url": "asd/schedule", "snippet_lines": ["        del self.jobs[:]\n"], "start_line": 80}, "39:43": {"description": "        \"\"\"Returns ``True`` if *other* is in this tokens ancestry.\"\"\"\n", "documentation": {"docs": "An iterable of document dictionaries."}, "filename": "solr/core.py", "related_lines": {"8,48": []}, "commit_hash": "ae72fb90b100090328aab1949ba978b1717795f5", "repo_url": "https://github.com/edsu/solrpy", "snippet_lines": ["        lst = [u'<add>']\n", "        for doc in docs:\n", "            self.__add(lst, doc)\n", "        lst.append(u'</add>')\n", "        return ''.join(lst)\n"], "start_line": 509}, "11:16": {"description": "    \"\"\"Parse a string or file-like object into a tree\"\"\"\n", "documentation": {}, "filename": "funcy/simple_funcs.py", "related_lines": {"37,66": []}, "commit_hash": "00863de7a8c5be2ac6503493e01f0b0281bab128", "repo_url": "https://github.com/Suor/funcy", "snippet_lines": ["    return lambda *a, **kw: func(*(args + a), **dict(kwargs, **kw))\n"], "start_line": 28}, "39:44": {"description": "        \"\"\"Return None.  Labels cells based on `func`.\n        If ``func(cell) is None`` then its datatype is\n        not changed; otherwise it is set to ``func(cell)``.\n", "documentation": {"docs": "An iterable of document dictionaries."}, "filename": "solr/core.py", "related_lines": {"8,48": []}, "commit_hash": "ae72fb90b100090328aab1949ba978b1717795f5", "repo_url": "https://github.com/edsu/solrpy", "snippet_lines": ["        lst = [u'<add>']\n", "        for doc in docs:\n", "            self.__add(lst, doc)\n", "        lst.append(u'</add>')\n", "        return ''.join(lst)\n"], "start_line": 509}, "22:18": {"description": "        \"\"\"Look for a sequence of bytes at the start of a string. If the bytes\n        are found return True and advance the position to the byte after the\n        match. Otherwise return False and leave the position alone\"\"\"\n", "documentation": {}, "filename": "micawber/compat.py", "related_lines": {"60,90": [], "149,177": [], "104,147": [], "35,55": []}, "commit_hash": "749f32825eedcdea69be9eb874d9ded0bd227dc3", "repo_url": "https://github.com/coleifer/micawber", "snippet_lines": ["            if key in self:\n", "                result = self[key]\n", "                del self[key]\n", "                return result\n", "            if default is self.__marker:\n", "                raise KeyError(key)\n", "            return default\n"], "start_line": 194}, "8:17": {"description": "        \"\"\"Attempts to detect at BOM at the start of the stream. If\n        an encoding can be determined from the BOM return the name of the\n        encoding otherwise return None\"\"\"\n", "documentation": {"disable": "\n    Disable a service.\n\n    ::\n\n        fabtools.systemd.disable('httpd')\n\n    .. note:: This function is idempotent.\n    ", "stop": "\n    Stop a service.\n\n    ::\n\n        if fabtools.systemd.is_running('foo'):\n            fabtools.systemd.stop('foo')\n\n    .. note:: This function is idempotent.\n    "}, "filename": "fabtools/systemd.py", "related_lines": {"4,30": []}, "commit_hash": "f7e851efaab057becce4ce0b8bd5fba146bf34c3", "repo_url": "https://github.com/ronnix/fabtools", "snippet_lines": ["    stop(service)\n", "    disable(service)\n"], "start_line": 132}, "31:32": {"description": "        Return the value at key ``name``, raises a KeyError if the key\n        doesn't exist.\n", "documentation": {"datetime.datetime.fromtimestamp": "timestamp[, tz] -> tz's local time from POSIX timestamp."}, "filename": "redis/client.py", "related_lines": {"5,58": []}, "commit_hash": "2ca3ffdc2102ecca1e7e419428706c29684ab38e", "repo_url": "https://github.com/andymccurdy/redis-py", "snippet_lines": ["    if not response:\n", "        return None\n", "    try:\n", "        response = int(response)\n", "    except ValueError:\n", "        return None\n", "    return datetime.datetime.fromtimestamp(response)\n"], "start_line": 47}, "8:50": {"description": "        Clear the query cache.\n\n        A simple helper that clears the internal query cache.\n", "documentation": {"disable": "\n    Disable a service.\n\n    ::\n\n        fabtools.systemd.disable('httpd')\n\n    .. note:: This function is idempotent.\n    ", "stop": "\n    Stop a service.\n\n    ::\n\n        if fabtools.systemd.is_running('foo'):\n            fabtools.systemd.stop('foo')\n\n    .. note:: This function is idempotent.\n    "}, "filename": "fabtools/systemd.py", "related_lines": {"4,30": []}, "commit_hash": "f7e851efaab057becce4ce0b8bd5fba146bf34c3", "repo_url": "https://github.com/ronnix/fabtools", "snippet_lines": ["    stop(service)\n", "    disable(service)\n"], "start_line": 132}, "50:25": {"description": "    Return a function that generates the toolbar tokens.\n", "documentation": {}, "filename": "tinydb/database.py", "related_lines": {"8,29": []}, "commit_hash": "bc2b630dd1dd996a4aa0de07decb4c7ea0190b0e", "repo_url": "https://github.com/msiemens/tinydb", "snippet_lines": ["        self._query_cache.clear()\n"], "start_line": 257}, "16:48": {"description": "    \"\"\"Prints alias for current shell.\"\"\"\n", "documentation": {"treebuilders.getTreeBuilder": "Get a TreeBuilder class for various types of tree with built-in support\n\n    treeType - the name of the tree type required (case-insensitive). Supported\n               values are:\n\n               \"dom\" - A generic builder for DOM implementations, defaulting to\n                       a xml.dom.minidom based implementation.\n               \"etree\" - A generic builder for tree implementations exposing an\n                         ElementTree-like interface, defaulting to\n                         xml.etree.cElementTree if available and\n                         xml.etree.ElementTree if not.\n               \"lxml\" - A etree-based builder for lxml.etree, handling\n                        limitations of lxml's implementation.\n\n    implementation - (Currently applies to the \"etree\" and \"dom\" tree types). A\n                      module implementing the tree type e.g.\n                      xml.etree.ElementTree or xml.etree.cElementTree.", "parse": "Parse a HTML document into a well-formed tree", "HTMLParser": "HTML parser. Generates a tree structure from a stream of (possibly\n        malformed) HTML"}, "filename": "html5lib/html5parser.py", "related_lines": {"7,53": []}, "commit_hash": "a3022dcea691780d300547bbf68b4dd921995d1c", "repo_url": "https://github.com/html5lib/html5lib-python", "snippet_lines": ["    tb = treebuilders.getTreeBuilder(treebuilder)\n", "    p = HTMLParser(tb, namespaceHTMLElements=namespaceHTMLElements)\n", "    return p.parse(doc, **kwargs)\n"], "start_line": 33}}